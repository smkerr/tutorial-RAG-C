{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smkerr/tutorial-rag-c/blob/main/Tutorial_RAG_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13i7KQ9t-CV8"
      },
      "source": [
        "# Tutorial: Retrieval Augmented Generation with Citation (RAG-C)\n",
        "## GRAD-E1394 Deep Learning -- Assignment 3\n",
        "Authors:\n",
        "*   Kai Foerster, k.foerster@students.hertie-school.org\n",
        "*   Amin Oueslati, a.oueslati@students.hertie-school.org\n",
        "*   Steven Kerr, s.kerr@students.hertie-school.org\n",
        "\n",
        "\n",
        "This tutorial offers a step-by-step guide on how to implement Retrieval Augmented Generation with Citation (RAG-C) to address the challenge of knowledge management in government. In essence, RAG-C provides LLMs with additional contextual information through an external data base, which significantly improves response accuracy and avoids hallucinations, particularly on highly domain-specific topics. After showcasing the enhanced performance of RAG-C on a toy example related to coffee prices in Berlin Mitte, this tutorial's focus is the development of a chatbot to answer questions on the US Federal Acquisition Regulation (FAR), the rule book for public procurement issuance in the United States. You will learn how to load and process FAR documents, store them in a data base and build your very own RAG-pipeline. To validate model performance, we leverage a FAR-quiz commonly used for training government employees, which allows us to compare responses from a standard LLM to the RAG-C enhanced LLM. Lastly, this tutorial will show you how to deploy your model as a Conversation User Interface (CUI) on a web server.\n",
        "\n",
        "An important aspect of this tutorial is its commitment to open-source accessibility, ensuring that all models and frameworks employed are available to everyone. Moreover, the tutorial is designed with computational efficiency in mind. Unlike the resource-heavy process of fine-tuning LLMs, the methods and applications showcased can be comfortably executed on standard laptops. Overall, the tutorial is designed to provide users with clear understanding and a hands-on policy application of how LLM can be augmented to improve their reliability, relevance and privacy conformity, without the need to fall back to paid LLM solutions.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNv0ANr5WcD_"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. [Memo](#memo)\n",
        "2. [Background & Prerequisites](#background-and-prereqs)\n",
        "3. [Software Requirements](#software-requirements)\n",
        "4. [Question-Answer LLM with Hugging Face](#question-answer-LLM)\n",
        "5. [Loading Data into a Vectorized Database](#vector-db)\n",
        "6. [RAG Pipeline](#rag-pipeline)\n",
        "7. [Model Evaluation](#model-evaluation)\n",
        "8. [Deployment as Conversation User Interface (CUI)](#cui-deployment)\n",
        "7. [References & Further Resources](#references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH81wjfsJsv1"
      },
      "source": [
        "<a id=\"memo\"></a>\n",
        "# 1. Memo\n",
        "\n",
        "Poor knowledge management is negatively impacting governments globally. In this regard, the very nature of government is often\n",
        "inextricably linked to the problem. Firstly, governments are enormous organizations, typically amounting to a country’s single largest employer. Secondly, governments are sparse and complex in their structure, stretching across hundreds of interdependent agencies and sub-agencies. Thirdly, the knowledge in government concerns highly technical matters, which change continuously as new laws are created or existing laws updated.\n",
        "\n",
        "The detrimental consequences of poor knowledge management in government are multifaceted. Resources are lost, as staff spends up to 20% of its weekly working hours on acquiring internal knowledge (Partnership for Public Service, 2019). Additionally, governments make suboptimal policy decisions, as information is siloed and not accounted for in decision-making processes. One of the most severe examples is 9/11, which likely could have been prevented, had there been better knowledge-sharing across US security agencies (9/11 Commission Report, 2004). Lastly, a state’s legitimacy heavily depends on the adherence to its governing rules: If its officials fail to respect them, due to poor knowledge management, this threatens a government’s democratic foundations.\n",
        "\n",
        "To tackle poor knowledge management in government, this tutorial introduces Retrieval Augmented Generation with Citations (RAG-C). Essentially, RAG-C is a method for providing Large Language Models (LLMs) with additional contextual information when answering questions. Thus, it is best understood as a conversational chatbot that has access to highly domain-specific knowledge. Importantly, RAG-C outperforms alternative solutions both in terms of trustworthiness and practicability. Firstly, the additional contextual information avoids hallucinations, while the citations enable users to manually validate the responses. Secondly, the RAG-C architecture is compatible with all LLMs, including any in-house models, and allows governments to store the contextual documents on its own servers. Thirdly, compared to fine-tuning the model, updating the data base is much more practical, as it requires limited technical expertise and is computationally inexpensive.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQgijl46pYzn"
      },
      "source": [
        "<a id=\"#background-and-prereqs\"></a>\n",
        "# 2. Background & Prerequisites\n",
        "\n",
        "A fundamental understanding of machine learning is required for this tutorial. Specifically, the reader should be familiar with LLMs, the vectorization of text data and similarity measures such as cosine similarity.\n",
        "\n",
        "Retrieval Augmented Generation with Citation (RAG-C) is a sophisticated process that marries the expansive knowledge of a LLM with the precision of targeted information retrieval. The process can be best explained with reference to the illustration below.\n",
        "\n",
        "The process starts with a user query, in this case the question: “How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?” Without the RAG-C component, this query would directly feed into the LLM which would generate a response (follow the arrows in red). For general knowledge questions or questions that the LLM has encountered during the training process, the responses are usually pretty good. The problem is, that most LLM have not been trained on data that we would like it to know, such as internal data from public institutions, more recent events that happened after the training process of the LLM or in this case the price of a Lebkuchen Latte. When one still insist to ask the LLM about such internal data, recent events or Lattes, it comes up with a false answers that are hallucinated. In scientific language, the question one asked is not in the parametric knowledge of the LLM, and hence the LLM invents knowledge.\n",
        "\n",
        "With RAG-C in contrast, the parametric knowledge of the LLM is augmented with source knowledge which is saved in a vectorized database. Essentially, we enable the LLM to retrieve additional relevant information beyond its parametric knowledge to answer a question. To understand how exactly the LLM is fed with the most relevant information to answer the question, let us look at the RAG-C pipeline represented in green arrows in the illustration below.\n",
        "\n",
        "In the RAG-C pipeline, the query is transformed into a numerical representation known as a query vector. This transformation is accomplished through an embedding model, which digests the textual query and outputs a high-dimensional vector that captures its semantic essence. The magic lies in the embedding model's ability to encode the meaning of the text into a mathematical form that can be compared and matched against the vectorized database. The query vector serves as a key to find relevant contexts within this database — contexts that are semantically close to the intent and content of the initial query. To find those texts, text vectors are compared on their similarity score, such as the cosine similarity score, and those contexts with the highest score are retrieved.\n",
        "\n",
        "The retrieved contexts are then fed to the LLM along with the original query text. This hybrid input, augmented with specific, relevant information, enables the LLM to generate responses that are not only contextually rich but also grounded in the retrieved data. The retrieved sources can further be added as references to the LLM response, bringing the 'C' (citation) element in RAG-C to light.\n",
        "\n",
        "When executing the code in this tutorial on Google Colab, ensure a smooth experience by downloading the contents of the `data` and `images` folders from the [tutorial repository](https://github.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c) and configuring your folder structure so that it mirrors that of the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "PBfppCu4VyTD",
        "outputId": "70ea4ded-85f6-414c-85ed-51fedc59e14a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/RAG%2BC_flowchart.jpg\" style=\"width: 55%\">"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Define the image path and the desired width (as a percentage of the cell width)\n",
        "image_path = 'https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/RAG%2BC_flowchart.jpg'\n",
        "image_width_percentage = 55  # e.g., 100% of the cell width\n",
        "\n",
        "# Create and display an HTML image tag with the specified width\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSRCNgYzUwaf"
      },
      "source": [
        "<a id=\"software-requirements\"></a>\n",
        "# 3. Software Requirements\n",
        "To run the first part of this tutorial up the the deployment in chainlit, it's necessary to have the following libraries installed. For a seamless experience, we suggest using Google Colab equipped with a GPU runtime, as it has most dependencies pre-installed, eliminating the need to configure them.\n",
        "\n",
        "For executing the notebook locally, it's advisable to have Python version 3.10 installed. One can use pipenv to install all below libraries into a new virtual environment which comes with the benefit that all dependencies are automatically configured by pip. [Here](https://pip.pypa.io/en/latest/) you find more information on how to use pip in case you are interested.\n",
        "\n",
        "These are the libraries you need to run the notebook:\n",
        "\n",
        "* langchain\n",
        "* sentence_transformers\n",
        "* chromadb\n",
        "* unstructured\n",
        "* chainlit\n",
        "* python-dotenv\n",
        "* bs4\n",
        "* tqdm\n",
        "* pandas\n",
        "* openpyxl\n",
        "* scikit-learn\n",
        "* numpy\n",
        "\n",
        "Furthermore, one needs to install ipykernel to run the notebook locally.\n",
        "These libraries can be installed on your local system as well as Colab by executing the subsequent cell (you will need to adapt the code slightly if you use pip).\n",
        "\n",
        "Please note, the prefix '!' might need to be substituted with '%' in the installation command, depending on your operating system or the configuration of your runtime environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jObV5LtDk0Vm",
        "outputId": "162372c0-cdd5-40d7-bf37-5d9435f5e4c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling langchain\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving langchain\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing langchain...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling sentence_transformers\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving sentence_transformers\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing sentence_transformers...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling chromadb\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving chromadb\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing chromadb...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling unstructured\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving unstructured\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing unstructured...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling chainlit\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving chainlit\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing chainlit...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling python-dotenv\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving python-dotenv\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing python-dotenv...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling bs4\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving bs4\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing bs4...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling tqdm\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving tqdm\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing tqdm...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling pandas\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving pandas\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing pandas...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling openpyxl\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving openpyxl\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing openpyxl...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling scikit-learn\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving scikit-learn\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing scikit-learn...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling numpy\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving numpy\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing numpy...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install  -qqq langchain\n",
        "!pip install  -qqq sentence_transformers\n",
        "!pip install  -qqq chromadb\n",
        "!pip install  -qqq unstructured\n",
        "!pip install  -qqq chainlit\n",
        "!pip install  -qqq python-dotenv\n",
        "!pip install  -qqq bs4\n",
        "!pip install  -qqq tqdm\n",
        "!pip install  -qqq pandas\n",
        "!pip install  -qqq openpyxl\n",
        "!pip install  -qqq scikit-learn\n",
        "!pip install  -qqq numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MumgSSs_VyTI",
        "outputId": "3b54e9fe-5931-43de-d147-709e8ffeb702"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# For managing API keys and secrets\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# For loading and manipulating LLMs\n",
        "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "# For loading data, embedding it and storing on ChromaDB\n",
        "from langchain.vectorstores import Chroma\n",
        "from bs4 import SoupStrainer\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import BSHTMLLoader\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# For evaluating the RAG using the quiz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Integrating images and video in the notebook\n",
        "from IPython.display import Image\n",
        "from IPython.display import YouTubeVideo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmiDgzHIVyTK"
      },
      "source": [
        "<a name=\"question-answer-LLM\"></a>\n",
        "# 4. Question & Answer LLM with Hugging Face\n",
        "\n",
        "This section introduces the application of large language models (LLMs) within the framework provided by Hugging Face. Setting up an operational LLM that the user can interact with is a pre-requiste for building a RAG enhanced LLM.  \n",
        "\n",
        "Despite the superior performance and ease of integration associated with models from commercial providers such as OpenAI, this notebook utilises Hugging Face, which provides an open-source alternative to state-of-the-art LLMs of commerical providers, which operate on a pay-as-you-go basis. More specifically, this tutorial will rely on the Falcon-7b-instruct model, an open-source model which comes well-balanced in size and performance. The Falcon-7b-instruct, developed by the Technology and Innovation Institute in the UAE, is a 7 billion parameter model fine-tuned for chat and instructions, ensuring high-quality responses [[1]](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
        "\n",
        "Employing the Falcon-7b-instruct through HuggingFaceHub eliminates the need for intensive local computing. This approach also circumvents the challenges associated with downloading and operating sizeable models on standard personal computing devices, a process often hindered by substantial working memory requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQKa6rwyVyTL"
      },
      "source": [
        "## Setting Up Hugging Face for Model Integration\n",
        "\n",
        "First, we need create a Hugging Face API key and save it into a .env file in our root directory. To do so, follow the below instructions:\n",
        "\n",
        "\n",
        "1. **Creating an Account on Hugging Face**:\n",
        "To create an account on Hugging Face, visit the [Hugging Face website](https://huggingface.co/join) and click the \"Sign Up\" button, typically located in the top right corner. Provide your email, and set a password. Follow any additional prompts, such as email verification, to complete your account registration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t9BsnObVyTL",
        "outputId": "639925a5-2b22-43dc-9687-4cca86045cbd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/kaifoerster/dl-tutorial/main/images/Huggingface1.PNG\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/Huggingface1.PNG'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeaFXRrIVyTM"
      },
      "source": [
        "2. **Creating an API Key on Hugging Face**:\n",
        "Once registered, log into your Hugging Face account and access your profile settings. Look for the \"API\" tab in the settings menu and click it. Here, create a new API key by clicking the “New API token” button, provide a name for the token, and then click “Create a token”. Remember to copy and securely store the generated API key, as it allows access to your Hugging Face account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXnmhPH4VyTM",
        "outputId": "84bcc1e1-a6fd-4b43-a014-d1efd09498fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/kaifoerster/dl-tutorial/main/images/Huggingface2.PNG\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/Huggingface2.PNG'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnP72CNvVyTN"
      },
      "source": [
        "3. **Saving the API Key in an .env File**:\n",
        "Create a new .env file in the root directory of your project using your code editor or IDE. Inside this file, enter the line HF_API_TOKEN=xxxxx, replacing xxxxx with your actual API key. Ensure you save the file after entering this information. If you are working on colab, you will need to upload the .env file to your files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW2pcQ_vVyTN",
        "outputId": "3c193ceb-0949-4bb7-a9ed-6bd33da50070"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/kaifoerster/dl-tutorial/main/images/envfile.PNG\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/envfile.PNG'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNgAnXC9VyTO"
      },
      "source": [
        "4. **Loading Environment Variables into the Notebook**:\n",
        "Now import the load_dotenv function from the dotenv package and call it to load your environment variables. Then, use import os followed by HF_API_TOKEN = os.getenv('HF_API_TOKEN') to access the Hugging Face API token you saved in your .env file. This process makes the API token available in your notebook for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbwZm6ReVyTO"
      },
      "outputs": [],
      "source": [
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve the 'HF_API_TOKEN' environment variable using os.getenv\n",
        "# This returns the value of 'HF_API_TOKEN' defined in the .env file\n",
        "HF_API_TOKEN = os.getenv('HF_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYO_oey-VyTO"
      },
      "source": [
        "## Loading the Hugging Face model in HuggingFaceHub\n",
        "\n",
        "Next, we specify the name of the model we want to load, provide our API token and make any further configurations to the model. Here, we have set:\n",
        "\n",
        "* **temperature**: This parameter influences the creativity of the LLM's responses, with a scale ranging from 0 to 1. A higher value results in more inventive outputs. We set this to 0.8, allowing for a degree of unpredictability or 'hallucination' in responses, which is a model behavior we aim to demonstrate to the user.\n",
        "\n",
        "* **max_lengt**: Defines the maximum length of the model's output, including both the input (such as the question and context) and the generated response. In this case, the limit is set to 1,000 tokens. This setting helps in managing the verbosity and relevance of the model's replies.\n",
        "\n",
        "* **use_cache**: When set to False, this parameter ensures that the model generates fresh responses for each query rather than relying on previously generated answers stored in cache. This is particularly useful for demonstrating the model's capability to generate unique responses on every run, which is essential for understanding its dynamic nature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT0GsXy6VyTP",
        "outputId": "21e7c897-21ba-450b-aecd-dc0b8efa4078"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Define the ID of the model to be used from Hugging Face Hub\n",
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# Initialize the HuggingFaceHub class with specified parameters\n",
        "# huggingfacehub_api_token: Uses the API token stored in the 'HF_API_TOKEN' environment variable\n",
        "# repo_id: Sets the repository ID to the specified model ID\n",
        "# model_kwargs: Sets additional parameters for the model as outlined above\n",
        "conv_model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=os.environ['HF_API_TOKEN'],\n",
        "    repo_id=model_id,\n",
        "    model_kwargs={\"temperature\":0.8,\"max_length\": 1000, \"use_cache\": False}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6eD7dRhVyTP"
      },
      "source": [
        "## Prompt Templates and LLMChain\n",
        "\n",
        "\n",
        "PromptTemplate from LangChain is a tool for creating structured prompts that are used to interact with language models. The structure and content of the prompt are designed to guide the language model in generating specific types of responses.\n",
        "\n",
        "In the code below, the PromptTemplate is being used to create a template for the language model, in this case, to act as a helpful assistant. The template includes a placeholder {human_message} where the user's query or statement will be inserted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DGNM4EUVyTP"
      },
      "outputs": [],
      "source": [
        "# Define a prompt template with a placeholder for user input.\n",
        "template = \"\"\"You are a helpful assistant that answers questions of the user.\n",
        "{human_message}\n",
        "\"\"\"\n",
        "\n",
        "# Create a PromptTemplate object, specifying 'human_message' as the dynamic input variable.\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"human_message\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r74SRVncVyTQ"
      },
      "source": [
        "This code snippet uses LLMCHain, a langchain function, to \"chain\" the language model and the prompt together into a pipeline. Using the LLMChain pipeline ensures that the prompt is fed into the LLM in the right way, so that it returns a response. The verbose=True parameter ensures that the function provides detailed output about the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz5p2FULVyTQ"
      },
      "outputs": [],
      "source": [
        "# Create a LLMChain object for a conversational model, using the specified LLM model and prompt template, with verbose logging.\n",
        "conv_chain = LLMChain(llm=conv_model, prompt=prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2TBLUjhVyTQ"
      },
      "source": [
        "## Response Generation\n",
        "\n",
        "Finally, the code below runs the model chain with a specific question about the cost of a Lebkuchen Latte at Pret a Manger in Berlin Mitte. The run method takes the user's question, inserts it into the {human_message} placeholder in the template, and then feeds the complete prompt to the language model. The model then generates a response based on this prompt, which is printed out.\n",
        "\n",
        "The choice of an uncommon drink here is intentional, as it aims to reveals a gap in the knowledge of the LLM. Notice that the model gives a fair estimate of how much a Lebkuchen Latte may cost at Pret a Manger in Berlin Mitte, but the price is actually false. Moreover, when rerunning the code several times, the model generates new price estimates. This example shows that the model hallucinates given it does not know the true price. In reality, a Lebkuchen Latte in Pret a Manger in Berlin costs 4.40 Euros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ok_YwJiVyTQ",
        "outputId": "68d62a30-4368-438f-9b98-6f5570c41926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
            "How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The price of a Lebkuchen Latte at Pret a Manger in Berlin Mitte is €3.50.\n"
          ]
        }
      ],
      "source": [
        "# Execute the conversational chain with a specific query and print the response.\n",
        "print(conv_chain.run(\"How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCSv4j8BVyTR"
      },
      "source": [
        "## Providing Relevant Context\n",
        "\n",
        "Next, to resolve the issue encountered above, we can provide the model with some context about the prices of coffees at Pret a Manger in Berlin Mitte. For this, an excerpt of the menu is added from [Uber Eats](https://www.ubereats.com/de/store/pret-a-manger/t2FefGKTXUavHMqkEDVCVw) below.\n",
        "\n",
        "First, we paste the excerpt as a multi-line string into a list called llmchain_information. The list is then converted into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_NtKOLiVyTR"
      },
      "outputs": [],
      "source": [
        "# List containing the menue of Pret a Manger in Berlin Mitte\n",
        "llmchain_information = [\n",
        "   \"\"\"Hot Drinks & Frappees - Pret A Manger - Berlin Mitte\n",
        "\n",
        "Latte - 1,5% Milch: 4,60 €\n",
        "Matcha Latte - Hafer: 5,40 €\n",
        "Flat White 0,1% Milch: 4,40 €\n",
        "Chai Latte - 1,5% Milch: 4,90 €\n",
        "Americano: 3,80 €\n",
        "Hot Chocolate - 1,5% Milch: 4,60 €\n",
        "Macchiato - 0,1% Milch: 2,65 €\n",
        "Espresso: 2,30 €\n",
        "Hot Chocolate - Hafer: 4,60 €\n",
        "Pumpkin Spice Latte - 0,1% Milch: 5,40 €\n",
        "Cappuccino - Hafer: 4,20 €\n",
        "Chai Latte - Soja: 4,90 €\n",
        "Espresso Doppio: 3,10 €\n",
        "Lebkuchen Latte: 4,40 € \"\"\"\n",
        "]\n",
        "\n",
        "# Converts the list of menu items into a single multi-line string.\n",
        "source_knowledge = \"\\n\".join(llmchain_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icf9Zb0-VyTR"
      },
      "source": [
        "Next, we adapt our template to add a placeholder for the menu we want to add as context to the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1iOH6fLVyTR"
      },
      "outputs": [],
      "source": [
        "# Template string defining the structure for interaction with an assistant, including context and user message.\n",
        "template_with_context = \"\"\"You are a helpful assistant that answers questions of the user, using the context provided below.\n",
        "\n",
        "Contexts:{source_knowledge}\n",
        "\n",
        "{human_message}\n",
        "\"\"\"\n",
        "\n",
        "# Creates an instance of PromptTemplate using the defined template and specifying input variables.\n",
        "prompt2 = PromptTemplate(template=template_with_context, input_variables=[\"human_message\", \"source_knowledge\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbKGdYE5VyTS"
      },
      "source": [
        "Here is the prompt printed out with the context and the question we want to ask!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3U75BcbVyTS",
        "outputId": "ee84cdca-7068-47d4-b3f4-81b473b17af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful assistant that answers questions of the user, using the context provided below.\n",
            "\n",
            "Contexts:Hot Drinks & Frappees - Pret A Manger - Berlin Mitte\n",
            "\n",
            "Latte - 1,5% Milch: 4,60 €\n",
            "Matcha Latte - Hafer: 5,40 €\n",
            "Flat White 0,1% Milch: 4,40 €\n",
            "Chai Latte - 1,5% Milch: 4,90 €\n",
            "Americano: 3,80 €\n",
            "Hot Chocolate - 1,5% Milch: 4,60 €\n",
            "Macchiato - 0,1% Milch: 2,65 €\n",
            "Espresso: 2,30 €\n",
            "Hot Chocolate - Hafer: 4,60 €\n",
            "Pumpkin Spice Latte - 0,1% Milch: 5,40 €\n",
            "Cappuccino - Hafer: 4,20 €\n",
            "Chai Latte - Soja: 4,90 €\n",
            "Espresso Doppio: 3,10 €\n",
            "Lebkuchen Latte: 4,40 € \n",
            "\n",
            "How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Formats and prints the prompt with specific user query and source knowledge context.\n",
        "print(prompt2.format(human_message=\"How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\", source_knowledge=source_knowledge))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjHzbE00VyTS"
      },
      "source": [
        "Next, we adapt the LLMChain to use the new prompt we just created!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AoqCtroVyTT"
      },
      "outputs": [],
      "source": [
        "# Initializes an LLMChain with a LLM  and the specified prompt template, enabling verbose output.\n",
        "context_chain = LLMChain(llm=conv_model, prompt=prompt2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5QoVItgVyTT"
      },
      "source": [
        "Finally, the code below runs the model chain with a specific question about the cost of a Lebkuchen Latte at Pret a Manger in Berlin Mitte with the menu as context. The resulting response of the model is spot on!\n",
        "\n",
        "What we have done conceptually by providing further context to the model is that we have augumented the parametric knowledge of the LLM with source knowledge. And what we have implemented manually for a specific questions, can be scaled up to thousands of documents when we use a RAG pipeline.\n",
        "\n",
        "The important concept to remember as we explain the more complex RAG pipeline next, is that, in essence, we are doing nothing more but providing the LLM with relevant source knowledge to augment its parametric knowledge as we are doing in the simple example about coffee prices at Pret a Manger in Berlin Mitte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsSMZHhNVyTT",
        "outputId": "9eceb4d3-9bc8-4282-bc14-b67613e7d00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user, using the context provided below.\n",
            "\n",
            "Contexts:Hot Drinks & Frappees - Pret A Manger - Berlin Mitte\n",
            "\n",
            "Latte - 1,5% Milch: 4,60 €\n",
            "Matcha Latte - Hafer: 5,40 €\n",
            "Flat White 0,1% Milch: 4,40 €\n",
            "Chai Latte - 1,5% Milch: 4,90 €\n",
            "Americano: 3,80 €\n",
            "Hot Chocolate - 1,5% Milch: 4,60 €\n",
            "Macchiato - 0,1% Milch: 2,65 €\n",
            "Espresso: 2,30 €\n",
            "Hot Chocolate - Hafer: 4,60 €\n",
            "Pumpkin Spice Latte - 0,1% Milch: 5,40 €\n",
            "Cappuccino - Hafer: 4,20 €\n",
            "Chai Latte - Soja: 4,90 €\n",
            "Espresso Doppio: 3,10 €\n",
            "Lebkuchen Latte: 4,40 € \n",
            "\n",
            "How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "4,40 €\n"
          ]
        }
      ],
      "source": [
        "# Executes the LLMChain with provided context and user message, and prints the result.\n",
        "print(context_chain.run({\n",
        "  'source_knowledge': source_knowledge,\n",
        "  'human_message': \"How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\"\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y7Tj833VyTT"
      },
      "source": [
        "<a name=\"vector-db\"></a>\n",
        "# 5. Loading Data into a Vectorized Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXoiLncsU3pe"
      },
      "source": [
        "<a name=\"data-description\"></a>\n",
        "# Data Description\n",
        "\n",
        "In this tutorial, we will build a RAG-C model capable of providing information on the Federal Acquisition Regulation (FAR) system which governs how U.S. government officials buy supplies and services. Our source data consists entirely of HTML files downloaded from [acquisition.gov](https://www.acquisition.gov/browse/index/far) where each HTML file represents a section or sub-section of the FAR regulations. In total, these HTML functions represent over 2,000 pages of rules and regulations consisting of nearly 4,000 separate subdivisions. Each subdivision comes with its own index (e.g., FAR 25.108-2) which codifies information on the precise part, subpart, section, and subsection corresponding with a given text as well as a subdivision title.\n",
        "\n",
        "This example is relevant to policymakers insofar as it demonstrates how LLMs can be used to provide government officials with accurate and timely information on complex regulations which need to be closely followed. Such a tool would augment the capacities of government officials, facilitate knowledge transfer to new staff, and keep long-time bureaucrats up-to-date on regulatory changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoF-BxSM5Jkc"
      },
      "source": [
        "## Data Download\n",
        "To retrieve the necessary data, proceed to [acquisition.gov](https://www.acquisition.gov/browse/index/far), then click on the HTML icon to download all HTML files as a zip file. Unzip this file to load all HTML files on your local device. If you are working on colab, create a new folder under \"Files\" named \"data\" and store the HTML files there. Please note that several of the these HTML files only contain metadata which are not immediately relevant to our task (e.g., Table of Contents), so please only keep files that adhere to the index format described above (e.g., \"6.204\" or \"25.108-2\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQoLw8kNVyTU",
        "outputId": "6a7b568e-9db0-412e-8528-0299e59b4cef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/kaifoerster/dl-tutorial/main/images/acquisition_gov.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/acquisition_gov.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSt6h_Q-oqjK"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Once the HTML files have been saved locally, we can proceed to the next step: data processing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5SabDtZVyTd"
      },
      "source": [
        "### Load documents\n",
        "\n",
        "We start by loading our HTML documents from the specified directory. We accomplish this by using LangChain's DirectoryLoader to convert the HTML files to LangChain Document objects which have properties we will leverage in subsequent steps. Note that we pass additional keyword arguments to the DirectoryLoader, allowing us to extract only paragraph elements from our HTML documents which is the content we are interested in. This has the added benefit of shortening the amount of time needed to load all our documents. In total, we load 3,487 documents, each corresponding to separate subdivisions of the FAR regulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nne1BfMVyTd",
        "outputId": "637347dd-f616-4545-f078-f319546e33f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 153/3487 [00:00<00:04, 759.65it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3487/3487 [00:04<00:00, 717.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3487 documents.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# define path to HTML files\n",
        "DATA_PATH = \"data/html\"\n",
        "\n",
        "# define Beautiful Soup key word args\n",
        "bs_kwargs = {\n",
        "    \"features\": \"html.parser\",\n",
        "    \"parse_only\": SoupStrainer(\"p\") # only extract paragraphs\n",
        "}\n",
        "\n",
        "# define Loader key word args\n",
        "loader_kwargs = {\n",
        "    \"open_encoding\": \"utf-8\",\n",
        "    \"bs_kwargs\": bs_kwargs\n",
        "}\n",
        "\n",
        "# define Loader\n",
        "loader = DirectoryLoader(\n",
        "    path=DATA_PATH,\n",
        "    glob=\"*.html\",\n",
        "    loader_cls=BSHTMLLoader,\n",
        "    loader_kwargs=loader_kwargs,\n",
        "    show_progress=True\n",
        "    )\n",
        "\n",
        "# load docs\n",
        "documents = loader.load()\n",
        "\n",
        "# status message\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuEmNwMBVyTe"
      },
      "source": [
        "Next, we implement some simple data cleaning procedures by removing line breaks, tabs, and excessive whitespace in order to (1) improve the appearance of the formatted text and (2) prevent excessive whitespace from contributing to the character limit of our query (discussed in more detail below). Additionally, we drop the title label from the metadata since we will not be using it in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y09kAWfmVyTe"
      },
      "outputs": [],
      "source": [
        "# clean up document content\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \") # remove line breaks and tabs\n",
        "    doc.page_content = re.sub(\"\\\\s+\", \" \", doc.page_content) # remove excessive whitespace\n",
        "    doc.metadata.pop(\"title\") # drop title labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mHI_vfTVyTe"
      },
      "source": [
        "We can inspect the first document to better understand the underlying structure of LangChain's Document class. Note that each Document consists of two sections: page_content and metadata. The page_content section is quite straightforward since it simply contains the text we just parsed from our HTML files. The metadata section contain a single label identifying the 'source' from which the document was retrieved. While this source label conveniently contains the text's FAR index number, it is formatted as a filepath which may lead to confusion on the part of the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDmIBocqVyTf",
        "outputId": "3905f123-8607-493f-bb1f-60ea216e8536"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Warranties of data shall be developed and used in accordance with agency regulations.', metadata={'source': 'data/html/46.708.html'})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtZNyjZvVyTf"
      },
      "source": [
        "### Modify Source Label\n",
        "\n",
        "A more intuitive way of displaying the source of a given text would be to show the FAR citation as well as the section title. This will tell the user not only where in the FAR to look for further information but also provide a brief description of what the section pertains to so that the user can immediately weigh the relevance of a suggested source. To update our source label, we repeat the process of loading all our documents, however, this time we only extract the titles which contain both the citation and title information. In doing so, we create a list of this information and use it to update the current source labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqyDmvjgVyTf",
        "outputId": "5af58fe4-706b-4d50-a1f6-9de1de01d25c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3487/3487 [00:02<00:00, 1236.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# define Beautiful Soup key word args\n",
        "bs_kwargs = {\n",
        "    \"features\": \"html.parser\",\n",
        "    \"parse_only\": SoupStrainer(\"title\") # only extract titles\n",
        "}\n",
        "\n",
        "# define Loader key word args\n",
        "loader_kwargs = {\n",
        "    \"open_encoding\": \"utf-8\",\n",
        "    \"bs_kwargs\": bs_kwargs\n",
        "}\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    path=DATA_PATH,\n",
        "    glob=\"*.html\",\n",
        "    loader_cls=BSHTMLLoader,\n",
        "    loader_kwargs=loader_kwargs,\n",
        "    show_progress=True\n",
        "    )\n",
        "\n",
        "document_titles = loader.load()\n",
        "\n",
        "# convert source metadata into a list\n",
        "title_list = [doc.metadata[\"title\"] for doc in document_titles]\n",
        "\n",
        "# update source labels\n",
        "i = 0\n",
        "for doc in documents:\n",
        "    doc.metadata[\"source\"] = \" \".join([\"FAR\", title_list[i]])\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LowOI-Q-VyTg"
      },
      "source": [
        "We can inspect our work to evaluate whether these source labels are more intuitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USfkbk5eVyTg",
        "outputId": "44de0c51-d175-4219-a6fd-54485ee1fe72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'source': 'FAR 46.708 Warranties of data.'},\n",
              " {'source': 'FAR 9.405 Effect of listing.'},\n",
              " {'source': 'FAR 11.106 Purchase descriptions for service contracts.'},\n",
              " {'source': 'FAR 16.204 Fixed-price incentive contracts.'},\n",
              " {'source': 'FAR 7.201 [Reserved]'}]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_metadata = [doc.metadata  for doc in documents]\n",
        "doc_metadata[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Df_U0sVyTg"
      },
      "source": [
        "## Create Vectorized Database\n",
        "\n",
        "Now that we have our documents loaded with metadata labels the way we want them to be, our next step is to store these documents in a vector database. These vector databases work by transforming documents into numerical vector representations using a text embedding model.\n",
        "\n",
        "For this tutorial, we use the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model made available by Hugging Face which maps texts to a 384 dimensional dense vector space. The original document is then stored along with its numerical vector representation and its metadata labels in the vector database. Compared to traditional databases which might return items based on a direct match with key search terms, a query made to a vector database is converted to a numerical vector using the same text embedding model applied to the documents stored within the database. The level of similarity between the numerical vector representation of the query and all the numerical vector representations of the documents is then computed. Typically, [cosine similarity](https://www.geeksforgeeks.org/cosine-similarity/) is the metric used to determine how similar two documents are to one another but [other methods do exist](https://towardsdatascience.com/5-data-similarity-metrics-f358a560855f) and several vector database providers have even developed their own methods for determining similarity.\n",
        "\n",
        "For this tutorial, we use [Chroma](https://www.trychroma.com/), an open-source vector database which [integrates well with LangChain](https://python.langchain.com/docs/integrations/vectorstores/chroma). In just a few lines of code, we specify the embedding model we would like Chroma to use ([all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)), specify the folder we would like our Chroma database files to be stored locally (note: the folder will be created if it does not already exist), and begin uploading our documents to our Chroma database. Please note that prior to creating our Chroma database, we remove any pre-existing Chroma files. By starting fresh each time, we ensure that we do not add duplicate documents to the same database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZj1iEYbVyTh",
        "outputId": "31ec2e63-ac6e-457c-9586-318f4323d8c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 3487 chunks to chroma_db.\n"
          ]
        }
      ],
      "source": [
        "# define embedding model\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "# load embedding function\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
        "\n",
        "# define file path for db\n",
        "CHROMA_PATH = \"chroma_db\"\n",
        "\n",
        "# first, clear out current db\n",
        "if os.path.exists(CHROMA_PATH):\n",
        "    shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "# initialize Chroma db and save locally\n",
        "db = Chroma.from_documents(\n",
        "    documents=documents, embedding=embedding_function, persist_directory=CHROMA_PATH\n",
        "    )\n",
        "db.persist()\n",
        "\n",
        "# status message\n",
        "print(f\"Saved {len(documents)} chunks to {CHROMA_PATH}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B13SmqZ7VyTh"
      },
      "source": [
        "To test whether our vector database is functioning properly, we provide it with a query and see whether Chroma returns the most relevant documents from our database. Here we ask it about the purpose of the FAR and use the `similarity_search_with_relevance_scores()` method with a minimum relevance score threshold of 50% while allowing for up to four documents to be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x352SrlVVyTh",
        "outputId": "d432a82b-f7f7-4994-e5d5-692f7f2f23f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(Document(page_content='The Federal Acquisition Regulations System is established for the codification and publication of uniform policies and procedures for acquisition by all executive agencies. The Federal Acquisition Regulations System consists of the Federal Acquisition Regulation (FAR), which is the primary document, and agency acquisition regulations that implement or supplement the FAR. The FAR System does not include internal agency guidance of the type described in 1.301(a)(2).', metadata={'source': 'FAR 1.101 Purpose.'}),\n",
              "  0.677825443885269),\n",
              " (Document(page_content='This part sets forth basic policies and general information about the Federal Acquisition Regulations System including purpose, authority, applicability, issuance, arrangement, numbering, dissemination, implementation, supplementation, maintenance, administration, and deviation. subparts 1.2,1.3, and 1.4 prescribe administrative procedures for maintaining the FAR System.', metadata={'source': 'FAR 1.000 Scope of part.'}),\n",
              "  0.6351669326334527)]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# query vector db\n",
        "query = \"What is the purpose of the Federal Acquisition Regulations?\"\n",
        "matching_docs = db.similarity_search_with_relevance_scores(\n",
        "    query=query,\n",
        "    k=4, # number of docs to return\n",
        "    score_threshold=.5 # min releavance score required\n",
        "    )\n",
        "matching_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNp-d5DQVyTi"
      },
      "source": [
        "While it may take some time to read through the documents it has returned in order to make assessment, the fact alone that the source label for the document which has been rated most relevant (with a score of 67.8%) contains \"purpose\" in its title is a good indication that our Chroma database is working as intended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5KXhWjEVyTi"
      },
      "source": [
        "<a name=\"rag-pipeline\"></a>\n",
        "# 6. RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpZtOD2dVyTi"
      },
      "source": [
        "Now that we have configured our vector database, we bring together the concepts we have covered so far to build our very own RAG pipeline. The idea is to create a function that will accept a question as input. Based on this question, our vector database will return the five documents calculated to be most relevant to the question at hand. Only documents with a relevance score greater than 50% will be returned. The content from these documents will then be pasted directly into the prompt given to our LLM. We automatically trim content that exceeds this threshold of 1000 words in order to speed up the inference process. Based on the prompt, our LLM should then provide us an informed response. This assumption only holds so long as the question is related to content contained in the vector database. As such, our RAG model will not be able to address questions outside of the scope of the FAR regulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrDJ2CkBVyTi"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n",
        "\n",
        "def RAG(query_text):\n",
        "\n",
        "    # search vector db\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=5, score_threshold=.5)\n",
        "    if len(results) == 0 or results[0][1] < 0.5:\n",
        "        print(f\"Unable to find matching results.\")\n",
        "\n",
        "    # add content from search results to prompt\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "    if len(context_text) > 1000: # handle character limit\n",
        "        context_text = context_text[:1000]\n",
        "        print(\"Warning: Context exceeded 1000 characters, trimming from the end.\")\n",
        "\n",
        "    # define prompt template\n",
        "    prompt_template=PromptTemplate(template=PROMPT_TEMPLATE, input_variables=[\"context\",  \"question\"])\n",
        "\n",
        "    # initialize LLMChain\n",
        "    chain = LLMChain(llm=conv_model, prompt=prompt_template, verbose=True)\n",
        "\n",
        "    # generate response based on context and question\n",
        "    response_text = chain.run({\"context\": context_text, \"question\": query_text})\n",
        "\n",
        "    # extract sources from search results\n",
        "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "\n",
        "    # format and print response with sources\n",
        "    formatted_response = f\"Response: {response_text} \\n Sources: {sources}\"\n",
        "    print(formatted_response)\n",
        "\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEkUIGxEVyTj"
      },
      "source": [
        "We can test our RAG pipeline by passing it a question. Here we ask the model to define the role of the Contracting Officer, a key stakeholder in the public procurement process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFQhUct0VyTj",
        "outputId": "34c8567d-7340-4b71-9ff4-df8831e71b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Context exceeded 1000 characters, trimming from the end.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Answer the question based only on the following context:\n",
            "\n",
            "The contracting officer shall cooperate with Department of Labor representatives in the examination of records, interviews with service employees, and all other aspects of investigations undertaken by the Department. When asked, agencies shall furnish the Wage and Hour Administrator or a designee, any available information on contractors, subcontractors, their contracts, and the nature of the contract services. The contracting officer shall promptly refer, in writing to the appropriate regional office of the Department, apparent violations and complaints received. Employee complaints shall not be disclosed to the employer.\n",
            "\n",
            "---\n",
            "\n",
            "Contracting officers are responsible for ensuring performance of all necessary actions for effective contracting, ensuring compliance with the terms of the contract, and safeguarding the interests of the United States in its contractual relationships. In order to perform these responsibilities, contracting officers should be allowed wide latitude to exercise bu\n",
            "\n",
            "---\n",
            "\n",
            "Answer the question based on the above context: What is a contracting officer?\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: A contracting officer is a government employee who is responsible for administering contracts with contractors or grantees in order to ensure compliance with the contract requirements. They are responsible for negotiating the terms and conditions of the contract, ensuring the timely performance of the contractor, and resolving disputes that may arise during the contract period. \n",
            " Sources: ['FAR 22.1024 Cooperation with the Department of Labor.', 'FAR 1.602-2 Responsibilities.', 'FAR 1.602-1 Authority.', 'FAR 42.601 General.', 'FAR 25.301-3 Weapons.']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'A contracting officer is a government employee who is responsible for administering contracts with contractors or grantees in order to ensure compliance with the contract requirements. They are responsible for negotiating the terms and conditions of the contract, ensuring the timely performance of the contractor, and resolving disputes that may arise during the contract period.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "RAG(\"What is a contracting officer?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX0zpbOZVyTj"
      },
      "source": [
        "Based on the output, the RAG model's output seems informative. The sources it cites appear reasonable, particularly at the start of the list which corresponds with the texts deemed most relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVg6BFRHVyTk"
      },
      "source": [
        "<a name=\"evaluation\"></a>\n",
        "# 7. Model Evaluation\n",
        "\n",
        "Due to the subject matter expertise required to properly evaluate our RAG model's performance, it is difficult to trust our own judgement when it comes to determining how effective the model is. In order to evaluate our model in a more or less objective way, we have sourced fifty questions and answers from training materials that were created with the express purpose of preparing government officials for their procurement licensing examinations. Only questions and answers directly relating to the FAR were included.\n",
        "\n",
        "Some questions and answers have been lightly edited to match the format expected by a chat bot. For example, one question which states \"This type of contract provides for the purchase of supplies or services for more than one, but not more than five program years\" has been re-worded as a question: \"What type of contract provides for the purchase of supplies or services for more than one, but not more than five program years?\" This is not intended to bias our evaluation but rather to make our questions compbatible with our open-source LLM.\n",
        "\n",
        "These question and answers have been saved to a spreadsheet which can be found under `data/xlsx/RAG_evaluation_input.xlsx`. Here we show a preview of the types of questions and answers are included in our evaluation. Note that columns for responses and similarity scores are currently empty. These will be populated automatically at a later stage of the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prlgAfM1VyTk",
        "outputId": "21356ea4-7a14-475b-a158-4c8d822328dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50 human-generated questions and answers\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>question_edited</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_edited</th>\n",
              "      <th>response_no_context</th>\n",
              "      <th>response_context</th>\n",
              "      <th>similarity_score_no_context</th>\n",
              "      <th>similarity_score_context</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True or False? Contractors who provide product...</td>\n",
              "      <td>Is it true that contractors who provide produc...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True or False? A contracting officer may unila...</td>\n",
              "      <td>Can a contracting officer unilaterally incorpo...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  True or False? Contractors who provide product...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  True or False? A contracting officer may unila...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                     question_edited  \\\n",
              "0  Is it true that contractors who provide produc...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  Can a contracting officer unilaterally incorpo...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                              answer  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...   \n",
              "1  FAR 1.102-2 provides that the principal custom...   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...   \n",
              "4  FAR 1.701 provides that a “Determination and F...   \n",
              "\n",
              "                                       answer_edited  response_no_context  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...                  NaN   \n",
              "1  FAR 1.102-2 provides that the principal custom...                  NaN   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...                  NaN   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...                  NaN   \n",
              "4  FAR 1.701 provides that a “Determination and F...                  NaN   \n",
              "\n",
              "   response_context  similarity_score_no_context  similarity_score_context  \\\n",
              "0               NaN                          NaN                       NaN   \n",
              "1               NaN                          NaN                       NaN   \n",
              "2               NaN                          NaN                       NaN   \n",
              "3               NaN                          NaN                       NaN   \n",
              "4               NaN                          NaN                       NaN   \n",
              "\n",
              "                                              source  \n",
              "0  https://publiccontractinginstitute.com/far-kno...  \n",
              "1  https://publiccontractinginstitute.com/far-kno...  \n",
              "2  https://publiccontractinginstitute.com/far-kno...  \n",
              "3  https://publiccontractinginstitute.com/far-kno...  \n",
              "4  https://publiccontractinginstitute.com/far-kno...  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load spreadsheet\n",
        "df = pd.read_excel('data/xlsx/RAG_evaluation_input.xlsx')\n",
        "print(f\"{df.shape[0]} human-generated questions and answers\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq9xEqvLVyTk"
      },
      "source": [
        "For each question, we generate both a response from the model *without* context and *with* context. The responses generated without context will serve as our baseline since it represents the response we would expect to have received in the absence of a RAG mechanism. Next, we apply the same text embedding model as we used for our vector database (i.e., [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)) to our human-generated answers to convert them to numerical vectors. We then do the same with the responses generated by the standalone LLM-model (baseline) and the RAG-C enhanced LLM. Using these embeddings, we can then compare these models to the ground truth by computing their respective cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMPGx6AFVyTk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# create list of questions\n",
        "question_list = df[\"question_edited\"]\n",
        "\n",
        "# generate baseline responses (not based on context)\n",
        "baseline_response_list = [conv_chain.run(question) for question in question_list]\n",
        "\n",
        "# generate responses (based on context)\n",
        "response_list = [RAG(question) for question in question_list]\n",
        "\n",
        "# create list of answers\n",
        "answer_list = df[\"answer_edited\"]\n",
        "\n",
        "# load embedding model\n",
        "embedding_model = SentenceTransformer(model_name_or_path=EMBED_MODEL)\n",
        "\n",
        "# convert answers to embeddings\n",
        "answer_embeddings = [embedding_model.encode(answer) for answer in answer_list]\n",
        "\n",
        "# compute embeddings for responses without context\n",
        "baseline_response_embeddings = []\n",
        "for response in baseline_response_list:\n",
        "    if response is None:\n",
        "        embedding = pd.NA\n",
        "    else:\n",
        "        embedding = embedding_model.encode(response)\n",
        "    baseline_response_embeddings.append(embedding)\n",
        "\n",
        "# compute embeddings for responses with context\n",
        "response_embeddings = []\n",
        "for response in response_list:\n",
        "    if response is None:\n",
        "        embedding = pd.NA\n",
        "    else:\n",
        "        embedding = embedding_model.encode(response)\n",
        "    response_embeddings.append(embedding)\n",
        "\n",
        "# reshape embeddings to 2D arrays (required by cosine_similarity)\n",
        "answer_embeddings = [np.reshape(embedding, (1, -1)) for embedding in answer_embeddings]\n",
        "response_embeddings = [np.reshape(embedding, (1, -1)) for embedding in response_embeddings]\n",
        "baseline_response_embeddings = [np.reshape(embedding, (1, -1)) for embedding in baseline_response_embeddings]\n",
        "\n",
        "# calculate similarity scores\n",
        "# questions vs responses with context\n",
        "similarity_scores = []\n",
        "for a_embedding, r_embedding in zip(answer_embeddings, response_embeddings):\n",
        "    if pd.isna(r_embedding).any():\n",
        "        score = pd.NA\n",
        "    else:\n",
        "        score = cosine_similarity(a_embedding, r_embedding).item()\n",
        "    similarity_scores.append(score)\n",
        "\n",
        "# questions vs responses without context\n",
        "baseline_similarity_scores = []\n",
        "for a_embedding, br_embedding in zip(answer_embeddings, baseline_response_embeddings):\n",
        "    if pd.isna(br_embedding).any():\n",
        "        score = pd.NA\n",
        "    else:\n",
        "        score = cosine_similarity(a_embedding, br_embedding).item()\n",
        "    baseline_similarity_scores.append(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2jCbQelVyTl"
      },
      "source": [
        "Lastly, we add the generated responses and corresponding similarity scores to our dataframe. Below you will find a glimpse of the populated dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5JZicWoVyTl",
        "outputId": "400e628b-cee6-4e64-e4f7-adb34c7abc47"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>question_edited</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_edited</th>\n",
              "      <th>response_no_context</th>\n",
              "      <th>response_context</th>\n",
              "      <th>similarity_score_no_context</th>\n",
              "      <th>similarity_score_context</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True or False? Contractors who provide product...</td>\n",
              "      <td>Is it true that contractors who provide produc...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>According to Federal Acquisition Regulation (F...</td>\n",
              "      <td>No</td>\n",
              "      <td>0.719175</td>\n",
              "      <td>0.084247</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>The principal customers for the products and s...</td>\n",
              "      <td>The Federal Acquisition System (FAS) serves as...</td>\n",
              "      <td>0.455638</td>\n",
              "      <td>0.244849</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True or False? A contracting officer may unila...</td>\n",
              "      <td>Can a contracting officer unilaterally incorpo...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>No, it is not common for a contracting officer...</td>\n",
              "      <td>As an AI language model, I cannot provide lega...</td>\n",
              "      <td>0.659838</td>\n",
              "      <td>0.645821</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>1. Secretary of Defense\\n2. Chief of Staff\\n3....</td>\n",
              "      <td>\\nThe three government officials responsible f...</td>\n",
              "      <td>0.509940</td>\n",
              "      <td>0.533886</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>I'm sorry, but I am not familiar with the term...</td>\n",
              "      <td>\\nThe question is ambiguous as it does not spe...</td>\n",
              "      <td>0.107604</td>\n",
              "      <td>0.183559</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  True or False? Contractors who provide product...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  True or False? A contracting officer may unila...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                     question_edited  \\\n",
              "0  Is it true that contractors who provide produc...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  Can a contracting officer unilaterally incorpo...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                              answer  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...   \n",
              "1  FAR 1.102-2 provides that the principal custom...   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...   \n",
              "4  FAR 1.701 provides that a “Determination and F...   \n",
              "\n",
              "                                       answer_edited  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...   \n",
              "1  FAR 1.102-2 provides that the principal custom...   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...   \n",
              "4  FAR 1.701 provides that a “Determination and F...   \n",
              "\n",
              "                                 response_no_context  \\\n",
              "0  According to Federal Acquisition Regulation (F...   \n",
              "1  The principal customers for the products and s...   \n",
              "2  No, it is not common for a contracting officer...   \n",
              "3  1. Secretary of Defense\\n2. Chief of Staff\\n3....   \n",
              "4  I'm sorry, but I am not familiar with the term...   \n",
              "\n",
              "                                    response_context  \\\n",
              "0                                                 No   \n",
              "1  The Federal Acquisition System (FAS) serves as...   \n",
              "2  As an AI language model, I cannot provide lega...   \n",
              "3  \\nThe three government officials responsible f...   \n",
              "4  \\nThe question is ambiguous as it does not spe...   \n",
              "\n",
              "   similarity_score_no_context  similarity_score_context  \\\n",
              "0                     0.719175                  0.084247   \n",
              "1                     0.455638                  0.244849   \n",
              "2                     0.659838                  0.645821   \n",
              "3                     0.509940                  0.533886   \n",
              "4                     0.107604                  0.183559   \n",
              "\n",
              "                                              source  \n",
              "0  https://publiccontractinginstitute.com/far-kno...  \n",
              "1  https://publiccontractinginstitute.com/far-kno...  \n",
              "2  https://publiccontractinginstitute.com/far-kno...  \n",
              "3  https://publiccontractinginstitute.com/far-kno...  \n",
              "4  https://publiccontractinginstitute.com/far-kno...  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add responses and similarity scores to df\n",
        "df[\"response_context\"] = response_list\n",
        "df[\"response_no_context\"] = baseline_response_list\n",
        "df[\"similarity_score_no_context\"] = baseline_similarity_scores\n",
        "df[\"similarity_score_context\"] = similarity_scores\n",
        "\n",
        "# save xlslx\n",
        "df.to_excel(\"data/xlsx/RAG_evaluation_output.xlsx\")\n",
        "\n",
        "# inspect df\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxw9ZqDGVyTm"
      },
      "source": [
        "## Results & Discussion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSsZs0ICVyTm",
        "outputId": "42c685c3-4621-4d7e-b539-4c22b364dbb8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>avg_similarity_score</th>\n",
              "      <th>response_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline</td>\n",
              "      <td>0.318718</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RAG</td>\n",
              "      <td>0.363935</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      model  avg_similarity_score  response_rate\n",
              "0  Baseline              0.318718          100.0\n",
              "1       RAG              0.363935          100.0"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create summary table\n",
        "summary_df = pd.DataFrame({\n",
        "    \"model\": [\"Baseline\", \"RAG\"],\n",
        "    \"avg_similarity_score\": [pd.Series(baseline_similarity_scores).mean(skipna=True),\n",
        "                             pd.Series(similarity_scores).mean(skipna=True)],\n",
        "    \"response_rate\": [(pd.Series(baseline_similarity_scores).count() / len(baseline_similarity_scores) * 100),\n",
        "                    (pd.Series(similarity_scores).count() / len(similarity_scores) * 100)]\n",
        "                    })\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZtGaJqGVyTn"
      },
      "source": [
        "By comparing the average similarity scores between the human-generated answer and our respective responses, we can see that the RAG model produces responses with a higher level of semantic similarity than the baseline model, supporting our hypothesis that additional external context improves the relevance of responses. While we cannot draw any definitive conclusions about the precision and accuracy of our RAG model using semantic similarity as an evaluation method, we can at least consider the model to be better-performing relative to the baseline.\n",
        "\n",
        "These results, and particularly the absolute similarity score should be treated with care. One concern relates to extent to which the similarity score also picks up differences in style, additional to information truth. Put differently, a model might get the information perfectly right in its answer, however, if it frames it vastly differently to the ground truth, the associated similarity score might still be low. However, since this dynamic should affect both the baseline and RAG model similarly, a relative comparison of the two still seems valid. Nevertheless, an extension of this evaluation would not only develop a broader set of validation questions, but should ideally also rely on human evaluators with domain knowledge to assess the accuracy of the models' answers.\n",
        "\n",
        "Independent of any concerns regarding accurate measurement, there exist obvious avenues to boost model performance. For instance, one might treat the separation of the FAR handbook into text chunks as a hyperparamter, whereby greater granularity could improve the relevance of content returned by the vector database, although this may come at the expense of losing additional context. More testing would be required to determine what level of granularity optimizes the quality of the model response. Furthermore, our pipeline is limited to using free and open-source models even while better LLMs do exist. A straightforward way of improving pipeline performance could be to use a paid option such as ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhkmytKNU_Z2"
      },
      "source": [
        "<a name=\"cui-deployment\"></a>\n",
        "# 8. Deployment as Conversation User Interface (CUI)\n",
        "\n",
        "Deploying the model as a Conversation User Interface (CUI) necessitates embedding the model in a deployment framework and running it on a web server. Given these technical intricacies, we cannot run the code as part of this notebook. Instead, we'll provide selected bits of the code and provide and overview of key considerations, as well as sharing the access to the repository from which we host the application for you to explore on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Fm2YhrVyTn"
      },
      "source": [
        "## Key Steps and Considerations\n",
        "\n",
        "- **Utilizing Chainlit Framework:** We employ Chainlit to convert the model into a web application. Chainlit, an asynchronous Python framework, supports numerous concurrent connections and operates on an event-driven basis. This setup allows for efficient processing and handling of user interactions. Using Chainlit's event-driven syntax, specific code segments are linked to particular user actions. For instance, when a user initiates a chat, the system loads and processes relevant documents, establishes the vector database, and loads the model. Subsequently, when a user poses a question, it feeds into the model as a query, computes a response, and presents associated sources.\n",
        "\n",
        "- **Adaptating RAG Pipeline for Compatibility with Chainlit:** To align with Chainlit's requirements, the model undergoes slight modifications, employing the `ConversationRetrievalChain`, which is highly tailored to the task. While this simplifies deployment, it sacrifices some flexibility and transparency compared to the earlier RAG versions you encountered. We made this choice consciously, as we wanted you to grasp what is happening under the hood first, before employing the more effective, but also opaque ready-made function.\n",
        "\n",
        "- **Chainlit Design Features:** Chainlit comes with many useful and intuitive design features. For instance, you may easily design and adapt your very own landing page accompanying your application. This can be done by manipulating the `chainlit.md` file which is created when running your application for the first time. Check out the landing page for our FAR-Chat in our repository!\n",
        "\n",
        "- **Deployment via HuggingFace Spaces:** Hosting the application on a personal web server restricts access. Therefore, we utilize HuggingFace Spaces, a free, open-source service from Hugging Face, for public deployment. The platform's interface resembles GitHub and simplifies the upload of data and applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDqFXgF8VyTn"
      },
      "source": [
        "## Chainlit Python Script\n",
        "The script below gives you a glimpse into the changes the pipeline has undergone when being transformed into a deployable python application. Please note, that the code chunk cannot be run as part of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LWo6UQzwj37",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "# import all necessary packages\n",
        "import os\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import BSHTMLLoader\n",
        "from bs4 import SoupStrainer\n",
        "import re\n",
        "\n",
        "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
        "\n",
        "import chainlit as cl\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# define prompt template\n",
        "system_template = \"\"\"Use the following pieces of context to answer the users question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "ALWAYS return a \"SOURCES\" part in your answer.\n",
        "The \"SOURCES\" part should be a reference to the source of the document from which you got your answer.\n",
        "And if the user greets with greetings like Hi, hello, How are you, etc reply accordingly as well.\n",
        "Example of your response should be:\n",
        "The answer is foo\n",
        "SOURCES: xyz\n",
        "Begin!\n",
        "----------------\n",
        "{summaries}\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "]\n",
        "prompt = ChatPromptTemplate.from_messages(messages)\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "\n",
        "# define the llm\n",
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "conv_model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=os.environ['HF_API_TOKEN'],\n",
        "    repo_id=model_id,\n",
        "    model_kwargs={\"temperature\":0.8,\"max_length\": 1000}\n",
        "    )\n",
        "\n",
        "# set up vector db with chroma\n",
        "data_path = \"data/html\"\n",
        "embed_model = \"all-MiniLM-L6-v2\" # Chroma defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# load documents\n",
        "def load_documents(directory):\n",
        "\n",
        "    # define Beautiful Soup key word args\n",
        "    bs_kwargs = {\n",
        "        \"features\": \"html.parser\",\n",
        "        \"parse_only\": SoupStrainer(\"p\") # only include relevant text\n",
        "        }\n",
        "\n",
        "    # define Loader key word args\n",
        "    loader_kwargs = {\n",
        "        \"open_encoding\": \"utf-8\",\n",
        "        \"bs_kwargs\": bs_kwargs\n",
        "        }\n",
        "\n",
        "    # define Loader\n",
        "    loader = DirectoryLoader(\n",
        "        path=directory,\n",
        "        glob=\"*.html\",\n",
        "        loader_cls=BSHTMLLoader,\n",
        "        loader_kwargs=loader_kwargs\n",
        "        )\n",
        "\n",
        "    documents = loader.load()\n",
        "    return documents\n",
        "\n",
        "\n",
        "# prepare documents\n",
        "def prepare_documents(documents):\n",
        "    for doc in documents:\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        doc.page_content = re.sub(\"\\\\s+\", \" \", doc.page_content)\n",
        "\n",
        "    # define Beautiful Soup key word args\n",
        "    bs_kwargs = {\n",
        "        \"features\": \"html.parser\",\n",
        "        \"parse_only\": SoupStrainer(\"title\") # only include relevant text\n",
        "        }\n",
        "\n",
        "    # define Loader key word args\n",
        "    loader_kwargs = {\n",
        "        \"open_encoding\": \"utf-8\",\n",
        "        \"bs_kwargs\": bs_kwargs\n",
        "        }\n",
        "\n",
        "    loader = DirectoryLoader(\n",
        "        path=data_path,\n",
        "        glob=\"*.html\",\n",
        "        loader_cls=BSHTMLLoader,\n",
        "        loader_kwargs=loader_kwargs\n",
        "        )\n",
        "\n",
        "    document_sources = loader.load()\n",
        "\n",
        "    # convert source metadata into a list\n",
        "    source_list = [doc.metadata[\"title\"] for doc in document_sources]\n",
        "\n",
        "    # update source metadata\n",
        "    i = 0\n",
        "    for doc in documents:\n",
        "        doc.metadata[\"source\"] = \" \".join([\"FAR\", source_list[i]])\n",
        "        i += 1\n",
        "    return documents\n",
        "\n",
        "# define a function to execute when a chat starts\n",
        "@cl.on_chat_start\n",
        "async def on_chat_start():\n",
        "    # instantiate the chain for that user session\n",
        "    embedding_func = SentenceTransformerEmbeddings(model_name=embed_model)\n",
        "\n",
        "    # display a message indicating document loading\n",
        "    msg = cl.Message(\n",
        "        content=\"Loading and processing documents. This may take a while...\",\n",
        "        disable_human_feedback=True)\n",
        "    await msg.send()\n",
        "\n",
        "    # load and prepare documents for processing\n",
        "    documents = load_documents(data_path)\n",
        "    documents = prepare_documents(documents)\n",
        "\n",
        "    # create a document search object asynchronously\n",
        "    docsearch = await cl.make_async(Chroma.from_documents)(\n",
        "        documents,\n",
        "        embedding_func\n",
        "    )\n",
        "\n",
        "    # initialize ChatMessageHistory object to store message history\n",
        "    message_history = ChatMessageHistory()\n",
        "\n",
        "    # initialize ConversationBufferMemory object to store conversation history\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        output_key=\"answer\",\n",
        "        chat_memory=message_history,\n",
        "        return_messages=True,\n",
        "    )\n",
        "\n",
        "    # create a ConversationalRetrievalChain object\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        conv_model,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=docsearch.as_retriever(),\n",
        "        memory=memory,\n",
        "        return_source_documents=True,\n",
        "    )\n",
        "\n",
        "    # indicate readiness for questions\n",
        "    msg.content = \"Ready. You can now ask questions!\"\n",
        "    await msg.update()\n",
        "\n",
        "    # store the chain in the user's session\n",
        "    cl.user_session.set(\"chain\", chain)\n",
        "\n",
        "# define a function to handle messages\n",
        "@cl.on_message\n",
        "async def main(message):\n",
        "    # retrieve the chain object from the user's session\n",
        "    chain = cl.user_session.get(\"chain\")  # type: ConversationalRetrievalChain\n",
        "    cb = cl.AsyncLangchainCallbackHandler()\n",
        "\n",
        "    # call the chain to process the incoming message\n",
        "    res = await chain.acall(message.content, callbacks=[cb])\n",
        "\n",
        "    # retrieve the answer and source documents from the chain's response\n",
        "    answer = res[\"answer\"]\n",
        "    source_documents = res[\"source_documents\"]\n",
        "\n",
        "    text_elements = []  # list to store text elements\n",
        "    source_names = set()  # set to store unique source names\n",
        "\n",
        "    # iterate through source documents and extract relevant information\n",
        "    for idx, source_doc in enumerate(source_documents):\n",
        "        source_name = source_doc.metadata[\"source\"]\n",
        "        text_elements.append(\n",
        "                cl.Text(content=source_doc.page_content,\n",
        "                        name=source_name))\n",
        "        source_names.add(source_name)  # add the source name to the set\n",
        "\n",
        "    # append sources information to the answer if available\n",
        "    if source_names:\n",
        "            answer += f\"\\nSources: {', '.join(source_names)}\"\n",
        "    else:\n",
        "            answer += \"\\nNo sources found\"\n",
        "\n",
        "    # send the answer along with any extracted text elements\n",
        "    await cl.Message(content=answer, elements=text_elements).send()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYDkQ79iVyTp"
      },
      "source": [
        "## Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsLy-IY2VyTq"
      },
      "source": [
        "#### Local Hosting:\n",
        "You can effortlessly host the application locally through the terminal. Just execute `chainlit run [filename.py] -w` in a terminal conencted to local repository. This command opens a locally hosted server which is displayed in your default internet browser. By including `-w`, any modifications made to the script will automatically update the associated image.\n",
        "\n",
        "#### Sharing via Web Server:\n",
        "For broader accessibility to the application, utilizing a web server becomes pivotal. Here's where HuggingFace Spaces comes in. The HuggingFace Space repository mirrors your local repository, containg the python application and any data. Additionally, you have to create a Docker file, which contains the instructions to build a Docker image, which in turn allows you to deploy your python application using containers. Importantly, the Docker file for chainlit deployment in HuggingFace Space is largely standardized, and you will only have to change the name of the Python file associated with your application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ZwlBNZVyTq"
      },
      "source": [
        "### HuggingFace Spaces Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pfxfh2KVyTq",
        "outputId": "6c043e6b-1e66-4af6-903a-2eddcaf05551"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/kaifoerster/dl-tutorial/main/images/hugging-face-space.PNG\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/hugging-face-space.PNG'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWQsJUG9VyTr"
      },
      "source": [
        "### Docker File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5XHyexbVyTr",
        "outputId": "5dbc450a-b1ed-4166-a01f-20fea58f62c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/kaifoerster/dl-tutorial/main/images/docker-file.PNG\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/Hertie-School-Deep-Learning-Fall-2023/tutorial-rag-c/main/images/docker-file.PNG'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJLZKhQHVyTr"
      },
      "source": [
        "## Access our Federal Acquisition Regulation (FAR) Chat\n",
        "Access our \"FAR-Chat\" on HuggingFace Spaces [here](https://huggingface.co/spaces/smkerr/rag-chat). Explore the repository and its files, especially the adapted Python application [here](https://huggingface.co/spaces/smkerr/rag-chat/tree/main)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGydMxgcVyTs"
      },
      "source": [
        "## Caveats on Usage and Performance\n",
        "\n",
        "- **Limitations of HuggingFace Spaces:** HuggingFace Spaces' free access comes with constrained CPU support. Consequently, working with the entire dataset of 3,400+ documents led to significant waiting times upon launching the chat. Thus, a random subset of 500 files was chosen to showcase functionality.\n",
        "\n",
        "- **Model Performance:** As a result of the smaller dataset and  performance limitations of open-source models like Falcon-7b-instruct compared to paid models like GPT-4, occasional erroneous or irrelevant answers might occur. Restarting the chat or rephrasing the question often rectifies such issues.\n",
        "\n",
        "- **Enhancing Performance:** A small investment in GPU-supported web hosting and premium model access could substantially improve performance, mitigating these limitations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKqewK_-ZLD"
      },
      "source": [
        "<a name=\"references\"></a>\n",
        "# References & Further Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in-ejZs9VyTs"
      },
      "source": [
        "## References\n",
        "\n",
        "The following resources were consulted along with ChatGPT and Stack Overflow to explain, troubleshoot, and comment on the code: (NOTE: Not sure whether this disclaimer is necessary)\n",
        "\n",
        "* Partnership for Public Service. 2019. Federal Workforce. Link: https://ourpublicservice.org/wp-content/uploads/2022/03/FedFigures_FY18-Workforce-1.pdf\n",
        "* 9/11 Comission. 2004. Final Report. Link: https://www.9-11commission.gov/report/911Report.pdf\n",
        "* Building RAG Chatbots with LangChain: https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb\n",
        "* Langchain RAG Tutorial: https://github.com/pixegami/langchain-rag-tutorial\n",
        "* Chatbots with RAG: LangChain Full Walkthrough: https://youtu.be/LhnCsygAvzY?si=bXJvLknL1Z5rIxR_\n",
        "* RAG + Langchain Python Project: https://youtu.be/tcqEUSNCn8I?si=xT0EdUocHfiiSO9z\n",
        "* Get Things Done with Prompt Engineering and LangChain: https://github.com/curiousily/Get-Things-Done-with-Prompt-Engineering-and-LangChain\n",
        "* Private Chatbot with Local LLM (Falcon 7B) and LangChain: https://www.mlexpert.io/prompt-engineering/chatbot-with-local-llm-using-langchain\n",
        "* Build a Private Chatbot with Local LLM (Falcon 7B) and LangChain: https://youtu.be/N7dGOUwufBM?si=c2htR20tc-1eC33V\n",
        "* Better Llama 2 with Retrieval Augmented Generation (RAG): https://youtu.be/ypzmPwLH_Q4?si=qVRzr4b95sMk_XBc\n",
        "* Learn LangChain In 1 Hour With End To End LLM Project With Deployment In Huggingface Spaces: https://youtu.be/qMIM7dECAkc?si=JcHClnCbKjjCwFKf\n",
        "* Best Open Source LLM — Falcon 40B Chatbot in LangChain: https://youtu.be/ukj_ITJKBwE?si=t-srzn5YgcCiLjiC\n",
        "* Hugging Face: setence-transformers: https://huggingface.co/sentence-transformers\n",
        "* Hugging Face: sentence-transformers/all-MiniLM-L6-v2: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "* Chroma Documentation: Hugging Face: https://docs.trychroma.com/embeddings/hugging-face\n",
        "* Chroma Documentation: https://docs.trychroma.com/embeddings\n",
        "* LangChain Documentation: Chroma: https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
        "* LangChain Documentation: Chat Models: https://python.langchain.com/docs/integrations/chat/\n",
        "* LangChain Documentation: File Directory: https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory\n",
        "* LangChain Documentation: HTML: https://python.langchain.com/docs/modules/data_connection/document_loaders/html#loading-html-with-beautifulsoup4\n",
        "* Embeddings and Vector Databases With ChromaDB: https://realpython.com/chromadb-vector-database/\n",
        "* Complete LangChain Tutorial: https://github.com/krishnaik06/Complete-Langchain-Tutorials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKHPEGh9VyTs"
      },
      "source": [
        "## Further Resources\n",
        "\n",
        "This tutorial is designed to give you a thorough introduction to RAG-Cs. While the tutorial focuses more on their hands-on implementation, these resources extend and deepen the content covered in the tutorial through conceptual underpinnings from academia and follow-up discussions.\n",
        "\n",
        "* [Seminal paper on RAG introduced at the 34th NeurIPS in 2020](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf): Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuettler, H., Lewis, M., Yih, W., Rocktaeschel, T., Riedel, S and Kiela, D. (2020)\n",
        "* [Here is a presentation of the above paper by the author](https://www.youtube.com/watch?v=JGpmQvlYRdU): Lewis, P. (2020)\n",
        "* [More on obstacles for RAG and potential remedies](https://aclanthology.org/2022.naacl-srw.7/): Yu, W. (2022)\n",
        "* [Blog post on evaluating RAG models](https://towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de): Besbes, A. (2023, Nov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHWzscJVyTt"
      },
      "source": [
        "This video gives an easy, hands-on introduction into RAG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icll8yL3VyTt",
        "outputId": "e28ad894-59e6-440d-a413-1976931d9d5b"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/LhnCsygAvzY\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x107b733d0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YouTubeVideo('LhnCsygAvzY')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}