{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smkerr/tutorial-rag-c/blob/main/Tutorial_RAG_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13i7KQ9t-CV8"
      },
      "source": [
        "# Tutorial: Retrieval Augmented Generation with Citation (RAG+C)\n",
        "## GRAD-E1394 Deep Learning\n",
        "\n",
        "Authors:\n",
        "*   Kai Foerster, k.foerster@students.hertie-school.org\n",
        "*   Amin Oueslati, a.oueslati@students.hertie-school.org\n",
        "*   Steven Kerr, s.kerr@students.hertie-school.org\n",
        "\n",
        "\n",
        "This tutorial offers a step-by-step guide on how to implement Retrieval Augmented Generation with Citation (RAG+C) to address the challenge of knowledge management in government. In essence, RAG+C provides Large Language Models (LLMs) with additional contextual information through an external data base, which significantly improves response accuracy and avoids hallucinations, particularly on highly domain-specific topics. After showcasing the enhanced performance of RAG+C on a toy example related to coffee prices in Berlin Mitte, this tutorial's focus is the development of a chatbot to answer questions on the U.S. Federal Acquisition Regulation (FAR), the rule book for public procurement within the United States federal government. You will learn how to load and process FAR documents, store them in a data base and build your very own RAG pipeline. To validate model performance, we leverage a FAR-quiz commonly used for training government employees, which allows us to compare responses from a standard LLM to the RAG+C enhanced LLM. Lastly, this tutorial will show you how to deploy your model as a Conversation User Interface (CUI) on a web server.\n",
        "\n",
        "An important aspect of this tutorial is its commitment to open-source accessibility, ensuring that all models and frameworks employed are available to everyone. Moreover, the tutorial is designed with computational efficiency in mind. Unlike the resource-heavy process of fine-tuning LLMs, the methods and applications showcased can be comfortably executed on standard laptops. Overall, the tutorial is designed to provide users with clear understanding and a hands-on policy application of how LLM can be augmented to improve their reliability, relevance and privacy conformity, without the need to rely on paid LLM solutions.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNv0ANr5WcD_"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. [Memo](#memo)\n",
        "2. [Background & Prerequisites](#background-and-prereqs)\n",
        "3. [Software Requirements](#software-requirements)\n",
        "4. [Question-Answer LLM with Hugging Face](#question-answer-LLM)\n",
        "5. [Loading Data into a Vector Database](#vector-db)\n",
        "6. [RAG Pipeline](#rag-pipeline)\n",
        "7. [Model Evaluation](#model-evaluation)\n",
        "8. [Deployment as Conversation User Interface (CUI)](#cui-deployment)\n",
        "7. [References & Further Resources](#references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH81wjfsJsv1"
      },
      "source": [
        "<a id=\"memo\"></a>\n",
        "# 1. Memo\n",
        "\n",
        "Poor knowledge management is negatively impacting governments globally. In this regard, the very nature of government is often\n",
        "inextricably linked to the problem. First, governments are enormous organizations, typically amounting to a country’s single largest employer. Second, governments are sparse and complex in their structure, stretching across hundreds of interdependent agencies and sub-agencies. Third, the knowledge in government concerns highly technical matters, which change continuously as new laws are created or existing laws updated.\n",
        "\n",
        "The detrimental consequences of poor knowledge management in government are multifaceted. Resources are lost, as staff spends up to 20% of its weekly working hours on acquiring internal knowledge (Partnership for Public Service, 2019). Additionally, governments make suboptimal policy decisions, as information is siloed and not accounted for in decision-making processes. One of the most severe examples is 9/11, which likely could have been prevented, had there been better knowledge-sharing across US security agencies (9/11 Commission Report, 2004). Lastly, a state’s legitimacy heavily depends on the adherence to its governing rules: If its officials fail to respect them, due to poor knowledge management, this threatens a government’s democratic foundations.\n",
        "\n",
        "To tackle poor knowledge management in government, this tutorial introduces Retrieval Augmented Generation with Citations (RAG+C). Essentially, RAG+C is a method for providing Large Language Models (LLMs) with additional contextual information when answering questions. Thus, it is best understood as a conversational chatbot that has access to highly domain-specific knowledge. Importantly, RAG+C outperforms alternative solutions both in terms of trustworthiness and practicability. First, the additional contextual information avoids hallucinations, while the citations enable users to manually validate the responses. Second, the RAG+C architecture is compatible with all LLMs, including any in-house models, and allows governments to store the contextual documents on its own servers. Third, compared to fine-tuning the model, updating the data base is much more practical, as it requires limited technical expertise and is computationally inexpensive.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQgijl46pYzn"
      },
      "source": [
        "<a id=\"#background-and-prereqs\"></a>\n",
        "# 2. Background & Prerequisites\n",
        "\n",
        "A fundamental understanding of machine learning is required for this tutorial. Specifically, the reader should be familiar with LLMs, the vectorization of text data and similarity measures such as cosine similarity.\n",
        "\n",
        "Retrieval Augmented Generation with Citations (RAG+C) is a sophisticated process that combines the expansive knowledge of an LLM with the precision of targeted information retrieval. The process can be best explained with reference to the illustration below.\n",
        "\n",
        "The process starts with a user query, in this case the question: “How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?” Without the RAG+C component, this query would directly feed into the LLM which would generate a response (follow the red arrows). For general knowledge questions or questions that the LLM has encountered during the training process, the responses are typically of good quality. The problem is, that most LLM have not been trained on data that we would like it to know, such as internal data from public institutions, more recent events that happened since the training process of the LLM, or in this case the price of a Lebkuchen Latte. When one still insists on asking the LLM about such internal data, recent events or Lattes, it generates false answers which are hallucinated. In scientific terms, hallucinations occur when the question one asks is not contained in the parametric knowledge of the LLM, and hence the LLM invents knowledge.\n",
        "\n",
        "In contrast to RAG+C, the parametric knowledge of the LLM is augmented with source knowledge which is saved in a vector database. Essentially, we enable the LLM to retrieve additional relevant information beyond its parametric knowledge to answer a question. To understand how exactly the LLM is fed with the most relevant information to answer the question, let us look at the RAG+C pipeline represented by the green arrow in the illustration below.\n",
        "\n",
        "In the RAG+C pipeline, the query is transformed into a numerical representation known as a query vector. This transformation is accomplished through an embedding model, which digests the textual query and outputs a high-dimensional vector that captures its semantic essence. The magic lies in the embedding model's ability to encode the meaning of the text into a mathematical form that can be compared and matched against the vector database. The query vector serves as a key to find relevant contexts within this database — contexts that are semantically close to the intent and content of the initial query. To find those texts, the similarity score, such as a cosine similarity score, between the query vector and each text vector in the vector database are computed. Those contexts with the highest score are retrieved.\n",
        "\n",
        "The retrieved contexts are then fed to the LLM along with the original query text. This hybrid input, augmented with specific, relevant information, enables the LLM to generate responses that are not only contextually rich but also grounded in the retrieved data. The retrieved sources can further be added as references to the LLM response, thus enabling the 'C' (citation) element in RAG+C.\n",
        "\n",
        "When executing the code in this tutorial on Google Colab, ensure a smooth experience by downloading the contents of the `data` and `img` folders from the [tutorial repository](https://github.com/smkerr/tutorial-RAG-C) and configuring your folder structure so that it mirrors that of the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "PBfppCu4VyTD",
        "outputId": "70ea4ded-85f6-414c-85ed-51fedc59e14a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/ragc-flowchart.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Define the image path and the desired width (as a percentage of the cell width)\n",
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/ragc-flowchart.png'\n",
        "image_width_percentage = 55  # e.g., 100% of the cell width\n",
        "\n",
        "# Create and display an HTML image tag with the specified width\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSRCNgYzUwaf"
      },
      "source": [
        "<a id=\"software-requirements\"></a>\n",
        "# 3. Software Requirements\n",
        "\n",
        "In order to execute the first part of this tutorial up until the deployment in chainlit, it is necessary to install the following libraries. For a seamless experience, we suggest using Google Colab equipped with a GPU runtime, as it already has most dependencies pre-installed, eliminating the need to configure dependencies on your own device.\n",
        "\n",
        "For executing the notebook locally, Python version 3.10 is advised. One can use pip to install all libraries listed below into a new virtual environment. [Here](https://pip.pypa.io/en/latest/) you find more information on how to use pip in case you are interested.\n",
        "\n",
        "These are the libraries you need to run the notebook:\n",
        "\n",
        "* bs4\n",
        "* chainlit\n",
        "* chromadb\n",
        "* langchain\n",
        "* numpy\n",
        "* openpyxl\n",
        "* pandas\n",
        "* python-dotenv\n",
        "* scikit-learn\n",
        "* sentence_transformers\n",
        "* tqdm\n",
        "* unstructured\n",
        "\n",
        "Furthermore, one needs to install ipykernel to run the notebook locally. These libraries can be installed on your local system as well as to Google Colab by executing the subsequent cell (you will need to adapt the code slightly if you use pip).\n",
        "\n",
        "Please note, the prefix '!' might need to be substituted with '%' in the installation command, depending on your operating system or the configuration of your runtime environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jObV5LtDk0Vm",
        "outputId": "162372c0-cdd5-40d7-bf37-5d9435f5e4c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling langchain\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving langchain\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing langchain...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling sentence_transformers\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving sentence_transformers\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing sentence_transformers...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling chromadb\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving chromadb\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing chromadb...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling unstructured\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving unstructured\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing unstructured...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling chainlit\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving chainlit\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing chainlit...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling python-dotenv\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving python-dotenv\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing python-dotenv...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling bs4\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving bs4\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing bs4...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling tqdm\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving tqdm\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing tqdm...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling pandas\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving pandas\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing pandas...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling openpyxl\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving openpyxl\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing openpyxl...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling scikit-learn\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving scikit-learn\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing scikit-learn...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
            "\u001b[1mLoading .env environment variables...\u001b[0m\n",
            "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
            "\u001b[1;32mInstalling numpy\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[?25lResolving numpy\u001b[33m...\u001b[0m\n",
            "\u001b[2K✔ Installation Succeeded\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Installing numpy...\n",
            "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1md3e772\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install  -qqq langchain\n",
        "!pip install  -qqq sentence_transformers\n",
        "!pip install  -qqq chromadb\n",
        "!pip install  -qqq unstructured\n",
        "!pip install  -qqq chainlit\n",
        "!pip install  -qqq python-dotenv\n",
        "!pip install  -qqq bs4\n",
        "!pip install  -qqq tqdm\n",
        "!pip install  -qqq pandas\n",
        "!pip install  -qqq openpyxl\n",
        "!pip install  -qqq scikit-learn\n",
        "!pip install  -qqq numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MumgSSs_VyTI",
        "outputId": "3b54e9fe-5931-43de-d147-709e8ffeb702"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# For managing API keys and secrets\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# For loading and manipulating LLMs\n",
        "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "# For loading data, embedding it and storing on ChromaDB\n",
        "from langchain.vectorstores import Chroma\n",
        "from bs4 import SoupStrainer\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import BSHTMLLoader\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# For evaluating the RAG using the quiz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Integrating images and video in the notebook\n",
        "from IPython.display import Image\n",
        "from IPython.display import YouTubeVideo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmiDgzHIVyTK"
      },
      "source": [
        "<a name=\"question-answer-LLM\"></a>\n",
        "# 4. Question & Answer LLM with Hugging Face\n",
        "\n",
        "This section introduces the application of large language models (LLMs) within the framework provided by Hugging Face. Setting up an operational LLM that the user can interact with is a pre-requiste for building a RAG enhanced LLM.  \n",
        "\n",
        "Despite the superior performance and ease of integration associated with models offered on a pay-as-you-go basis by commercial providers such as OpenAI, this notebook utilises Hugging Face, which provides an open-source alternative to state-of-the-art LLMs of commerical providers, which operate on a pay-as-you-go basis. This tutorial uses the falcon-7b-instruct model, an open-source model which comes well-balanced in size and performance. The falcon-7b-instruct model, developed by the Technology and Innovation Institute in the UAE, is a 7 billion parameter model fine-tuned for chat and instructions, ensuring high-quality responses [[1]](https://huggingface.co/tiiuae/falcon-7b-instruct).\n",
        "\n",
        "Employing the falcon-7b-instruct model through the Hugging Face Hub eliminates the need for intensive local computing. This approach also circumvents the challenges associated with downloading and operating sizeable models on standard personal computing devices, a process often hindered by substantial working memory requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQKa6rwyVyTL"
      },
      "source": [
        "## Setting Up Hugging Face for Model Integration\n",
        "\n",
        "First, we need create a Hugging Face API key and save it to a .env file in our root directory. The instructions below will walk you through this process:\n",
        "\n",
        "\n",
        "1. **Creating an Account on Hugging Face**:\n",
        "To create an account on Hugging Face, visit the [Hugging Face website](https://huggingface.co/join) and click the \"Sign Up\" button, typically located in the top right-hand corner. Provide your email, and set your password. Follow any additional prompts, such as email verification, to complete your account registration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4t9BsnObVyTL",
        "outputId": "639925a5-2b22-43dc-9687-4cca86045cbd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/hugging-face-login.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/hugging-face-login.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeaFXRrIVyTM"
      },
      "source": [
        "2. **Creating an API Key on Hugging Face**:\n",
        "Once registered, log into your Hugging Face account and access your profile settings. Click on the \"API\" tab in the settings menu and click it. Next, create a new API key by clicking the “New API token” button, provide a name for the token, and then click “Create a token”. Remember to copy and securely store the generated API key, as it allows access to your Hugging Face account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qXnmhPH4VyTM",
        "outputId": "84bcc1e1-a6fd-4b43-a014-d1efd09498fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/hugging-face-token.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/hugging-face-token.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnP72CNvVyTN"
      },
      "source": [
        "3. **Saving the API Key in an .env File**:\n",
        "Create a new .env file in the root directory of your project using your code editor or IDE. Inside this file, enter the line `HF_API_TOKEN=\"xxxxx\"`, replacing `xxxxx` with your actual API key. Ensure you save the file after entering this information. If you are working on colab, you will need to upload the .env file to your files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cW2pcQ_vVyTN",
        "outputId": "3c193ceb-0949-4bb7-a9ed-6bd33da50070"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/envfile.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/envfile.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNgAnXC9VyTO"
      },
      "source": [
        "4. **Loading Environment Variables into the Notebook**:\n",
        "Now import the `load_dotenv` function from the dotenv package and call it to load your environment variables. Then, use `import os` followed by `HF_API_TOKEN = os.getenv('HF_API_TOKEN')` to access the Hugging Face API token you saved in your .env file. This process makes the API token available in your notebook for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbwZm6ReVyTO"
      },
      "outputs": [],
      "source": [
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve the 'HF_API_TOKEN' environment variable using os.getenv\n",
        "# This returns the value of 'HF_API_TOKEN' defined in the .env file\n",
        "HF_API_TOKEN = os.getenv('HF_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYO_oey-VyTO"
      },
      "source": [
        "## Loading the Hugging Face model in Hugging Face Hub\n",
        "\n",
        "Next, we specify the name of the model we want to load, provide our API token and make any further configurations to the model. At this stage, we must decide upon the following hyperparameters:\n",
        "\n",
        "* **temperature**: This parameter influences the creativity of the LLM's responses, with a scale ranging from 0 to 1. A higher value results in more inventive outputs. We set this to 0.8, allowing for a degree of unpredictability or 'hallucination' in responses, which is a model behavior we aim to demonstrate to the user.\n",
        "\n",
        "* **max_length**: Defines the maximum length of the model's output, including both the input (such as the question and context) and the generated response. In this case, the limit is set to 1,000 tokens since this is the maximum prompt length allowed by the falcon-7b-instruct model. This setting helps in managing the verbosity and relevance of the model's replies.\n",
        "\n",
        "* **use_cache**: When set to `False`, this parameter ensures that the model generates fresh responses for each query rather than relying on previously generated answers stored in cache. This is particularly useful for demonstrating the model's capability to generate unique responses on every run, which is essential for understanding its dynamic nature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT0GsXy6VyTP",
        "outputId": "21e7c897-21ba-450b-aecd-dc0b8efa4078"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Define the ID of the model to be used from Hugging Face Hub\n",
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# Initialize the HuggingFaceHub class with specified parameters\n",
        "# huggingfacehub_api_token: Uses the API token stored in the 'HF_API_TOKEN' environment variable\n",
        "# repo_id: Sets the repository ID to the specified model ID\n",
        "# model_kwargs: Sets additional parameters for the model as outlined above\n",
        "conv_model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=os.environ['HF_API_TOKEN'],\n",
        "    repo_id=model_id,\n",
        "    model_kwargs={\"temperature\":0.8,\"max_length\": 1000, \"use_cache\": False}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6eD7dRhVyTP"
      },
      "source": [
        "## Prompt Templates and LLMChain\n",
        "\n",
        "\n",
        "`PromptTemplate` from LangChain is a tool for creating structured prompts that are used to interact with language models. The structure and content of the prompt are designed to guide the language model towards generating specific types of responses.\n",
        "\n",
        "In the code below, the `PromptTemplate` is being used to create a template for the language model, in this case, to act as a helpful assistant. The template includes a placeholder `{human_message}` where the user's query or statement will be inserted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DGNM4EUVyTP"
      },
      "outputs": [],
      "source": [
        "# Define a prompt template with a placeholder for user input.\n",
        "template = \"\"\"You are a helpful assistant that answers questions of the user.\n",
        "{human_message}\n",
        "\"\"\"\n",
        "\n",
        "# Create a PromptTemplate object, specifying 'human_message' as the dynamic input variable.\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"human_message\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r74SRVncVyTQ"
      },
      "source": [
        "This code snippet uses `LLMChain`, a langchain function, to \"chain\" the language model and the prompt together into a single pipeline. Using the `LLMChain` pipeline ensures that the prompt is fed into the LLM in an accepted format, so that we receive a response. The `verbose=True` parameter ensures that the function provides detailed output about the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz5p2FULVyTQ"
      },
      "outputs": [],
      "source": [
        "# Create a LLMChain object for a conversational model, using the specified LLM model and prompt template, with verbose logging.\n",
        "conv_chain = LLMChain(llm=conv_model, prompt=prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2TBLUjhVyTQ"
      },
      "source": [
        "## Response Generation\n",
        "\n",
        "Finally, the code below runs the model chain with a specific question about the cost of a Lebkuchen Latte at Pret a Manger in Berlin Mitte. The `run()` method takes the user's question, inserts it into the `{human_message}` placeholder in the prompt template, and then feeds the complete prompt to the LLM. The model then generates a response based on this prompt, which is printed out.\n",
        "\n",
        "The choice of an uncommon drink here is intentional, with the aim of exposing a gap in the knowledge of the LLM's knowledge base. Notice that while the model gives a fair estimate of how much a Lebkuchen Latte may cost at Pret a Manger in Berlin Mitte, but the price is actually false. Moreover, each time we re-run the code, the model generates a new price estimate. This example shows that the model hallucinates given it does not know the true price. In actuality, a Lebkuchen Latte at Pret a Manger in Berlin costs 4.40 Euros at the time of writing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ok_YwJiVyTQ",
        "outputId": "68d62a30-4368-438f-9b98-6f5570c41926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
            "How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The price of a Lebkuchen Latte at Pret a Manger in Berlin Mitte is €3.50.\n"
          ]
        }
      ],
      "source": [
        "# Execute the conversational chain with a specific query and print the response.\n",
        "print(conv_chain.run(\"How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCSv4j8BVyTR"
      },
      "source": [
        "## Providing Relevant Context\n",
        "\n",
        "To resolve the issue encountered above, we can provide the model with some context about the current menu prices for Pret a Manger in Berlin Mitte. We implement this by pasting an excerpt of the menu obtained from [Uber Eats](https://www.ubereats.com/de/store/pret-a-manger/t2FefGKTXUavHMqkEDVCVw) as a multi-line string into a list called `llmchain_information`. The list is then converted into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_NtKOLiVyTR"
      },
      "outputs": [],
      "source": [
        "# List containing the menue of Pret a Manger in Berlin Mitte\n",
        "llmchain_information = [\n",
        "   \"\"\"Hot Drinks & Frappees - Pret A Manger - Berlin Mitte\n",
        "\n",
        "Latte - 1,5% Milch: 4,60 €\n",
        "Matcha Latte - Hafer: 5,40 €\n",
        "Flat White 0,1% Milch: 4,40 €\n",
        "Chai Latte - 1,5% Milch: 4,90 €\n",
        "Americano: 3,80 €\n",
        "Hot Chocolate - 1,5% Milch: 4,60 €\n",
        "Macchiato - 0,1% Milch: 2,65 €\n",
        "Espresso: 2,30 €\n",
        "Hot Chocolate - Hafer: 4,60 €\n",
        "Pumpkin Spice Latte - 0,1% Milch: 5,40 €\n",
        "Cappuccino - Hafer: 4,20 €\n",
        "Chai Latte - Soja: 4,90 €\n",
        "Espresso Doppio: 3,10 €\n",
        "Lebkuchen Latte: 4,40 € \"\"\"\n",
        "]\n",
        "\n",
        "# Converts the list of menu items into a single multi-line string.\n",
        "source_knowledge = \"\\n\".join(llmchain_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icf9Zb0-VyTR"
      },
      "source": [
        "Next, we adapt our template by adding a placeholder for the menu prices that we want to add as context to the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1iOH6fLVyTR"
      },
      "outputs": [],
      "source": [
        "# Template string defining the structure for interaction with an assistant, including context and user message.\n",
        "template_with_context = \"\"\"You are a helpful assistant that answers questions of the user, using the context provided below.\n",
        "\n",
        "Contexts:{source_knowledge}\n",
        "\n",
        "{human_message}\n",
        "\"\"\"\n",
        "\n",
        "# Creates an instance of PromptTemplate using the defined template and specifying input variables.\n",
        "prompt2 = PromptTemplate(template=template_with_context, input_variables=[\"human_message\", \"source_knowledge\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbKGdYE5VyTS"
      },
      "source": [
        "Here is our prompt with the context and the question included!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3U75BcbVyTS",
        "outputId": "ee84cdca-7068-47d4-b3f4-81b473b17af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful assistant that answers questions of the user, using the context provided below.\n",
            "\n",
            "Contexts:Hot Drinks & Frappees - Pret A Manger - Berlin Mitte\n",
            "\n",
            "Latte - 1,5% Milch: 4,60 €\n",
            "Matcha Latte - Hafer: 5,40 €\n",
            "Flat White 0,1% Milch: 4,40 €\n",
            "Chai Latte - 1,5% Milch: 4,90 €\n",
            "Americano: 3,80 €\n",
            "Hot Chocolate - 1,5% Milch: 4,60 €\n",
            "Macchiato - 0,1% Milch: 2,65 €\n",
            "Espresso: 2,30 €\n",
            "Hot Chocolate - Hafer: 4,60 €\n",
            "Pumpkin Spice Latte - 0,1% Milch: 5,40 €\n",
            "Cappuccino - Hafer: 4,20 €\n",
            "Chai Latte - Soja: 4,90 €\n",
            "Espresso Doppio: 3,10 €\n",
            "Lebkuchen Latte: 4,40 € \n",
            "\n",
            "How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Formats and prints the prompt with specific user query and source knowledge context.\n",
        "print(prompt2.format(human_message=\"How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\", source_knowledge=source_knowledge))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjHzbE00VyTS"
      },
      "source": [
        "Next, we adapt the `LLMChain` by passing it the new prompt we just created!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AoqCtroVyTT"
      },
      "outputs": [],
      "source": [
        "# Initializes an LLMChain with a LLM  and the specified prompt template, enabling verbose output.\n",
        "context_chain = LLMChain(llm=conv_model, prompt=prompt2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5QoVItgVyTT"
      },
      "source": [
        "Finally, the code below runs the model chain with our specific question about the cost of a Lebkuchen Latte at Pret a Manger in Berlin Mitte and with the menu prices included as context. The resulting response of the model is spot on!\n",
        "\n",
        "By providing further context to the model, we have augumented the parametric knowledge of the LLM with source knowledge. And the process we have implemented manually for our specific question can be scaled up to thousands of documents when we use a RAG pipeline.\n",
        "\n",
        "The important concept to remember when it comes to building this RAG pipeline is that, in essence, we are doing nothing more than providing the LLM with relevant source knowledge to augment its parametric knowledge as we are doing in the simple example about coffee prices at Pret a Manger in Berlin Mitte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsSMZHhNVyTT",
        "outputId": "9eceb4d3-9bc8-4282-bc14-b67613e7d00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user, using the context provided below.\n",
            "\n",
            "Contexts:Hot Drinks & Frappees - Pret A Manger - Berlin Mitte\n",
            "\n",
            "Latte - 1,5% Milch: 4,60 €\n",
            "Matcha Latte - Hafer: 5,40 €\n",
            "Flat White 0,1% Milch: 4,40 €\n",
            "Chai Latte - 1,5% Milch: 4,90 €\n",
            "Americano: 3,80 €\n",
            "Hot Chocolate - 1,5% Milch: 4,60 €\n",
            "Macchiato - 0,1% Milch: 2,65 €\n",
            "Espresso: 2,30 €\n",
            "Hot Chocolate - Hafer: 4,60 €\n",
            "Pumpkin Spice Latte - 0,1% Milch: 5,40 €\n",
            "Cappuccino - Hafer: 4,20 €\n",
            "Chai Latte - Soja: 4,90 €\n",
            "Espresso Doppio: 3,10 €\n",
            "Lebkuchen Latte: 4,40 € \n",
            "\n",
            "How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "4,40 €\n"
          ]
        }
      ],
      "source": [
        "# Executes the LLMChain with provided context and user message, and prints the result.\n",
        "print(context_chain.run({\n",
        "  'source_knowledge': source_knowledge,\n",
        "  'human_message': \"How much does a Lebkuchen Latte cost at Pret a Manger in Berlin Mitte?\"\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y7Tj833VyTT"
      },
      "source": [
        "<a name=\"vector-db\"></a>\n",
        "# 5. Loading Data into a Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXoiLncsU3pe"
      },
      "source": [
        "<a name=\"data-description\"></a>\n",
        "# Data Description\n",
        "\n",
        "In the next part of this tutorial, we will build a RAG+C pipeline applied to a public policy use case. Specifically, we will create a RAG+C chatbot capable of answering questions about the Federal Acquisition Regulation (FAR) system which governs how U.S. federal government officials procure supplies and services. Our source data consists entirely of HTML files downloaded from [acquisition.gov](https://www.acquisition.gov/browse/index/far) where each HTML file represents either a section or sub-section of the FAR regulations. In total, we have nearly 3,5000 separate HTML files, each corresponding with a separate subdivision, amounting to well over 2,000 pages of rules and regulations. Each subdivision comes with its own title and reference number (e.g., FAR 25.108-2). This reference number encodes information on the part, subpart, section, and subsection where the text is located.\n",
        "\n",
        "This example is relevant to policymakers insofar as it demonstrates how LLMs can be used to provide government officials with accurate and timely information on complex regulations which must be closely followed. Such a tool would augment the capacities of government officials, facilitate knowledge transfer to new staff, and keep long-time bureaucrats up-to-date on regulatory changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoF-BxSM5Jkc"
      },
      "source": [
        "## Data Download\n",
        "You can retrieve the necessary data by proceeding to [acquisition.gov](https://www.acquisition.gov/browse/index/far) and clicking on the HTML icon to download all HTML files as a zip file. Unzip the file to load all HTML files. If you are working on Google Colab, then create a new folder under \"Files\" named \"data\" and store the HTML files there. Please note that several of the these HTML files only contain metadata which are not immediately relevant to our task (e.g., Table of Contents for each FAR part), so we recommend you only keep files that align with to the reference number format described above (e.g., \"6.204\", \"25.108-2\", etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EQoLw8kNVyTU",
        "outputId": "6a7b568e-9db0-412e-8528-0299e59b4cef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/acquisition-gov.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/acquisition-gov.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSt6h_Q-oqjK"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Once the HTML files have been saved locally, we can proceed to the next step: data processing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5SabDtZVyTd"
      },
      "source": [
        "### Load documents\n",
        "\n",
        "We start by loading our HTML documents from the directory where they have been stored. We use LangChain's `DirectoryLoader` to convert the HTML files to LangChain `Document` objects which have some useful properties that we will leverage in subsequent steps. Since the content we are interested in extracting is contained in the paragraph elements of our HTML documents, we pass `\"parse_only: SoupStrainer(\"p\")` as a keyword argument to our loader, so that only this specific content is returned. This has the added benefit of shortening the amount of time required to load all our documents. \n",
        "\n",
        "In total, we load 3,487 documents, each corresponding with a separate section or sub-section of the FAR regulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nne1BfMVyTd",
        "outputId": "637347dd-f616-4545-f078-f319546e33f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 153/3487 [00:00<00:04, 759.65it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3487/3487 [00:04<00:00, 717.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3487 documents.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# define path to HTML files\n",
        "DATA_PATH = \"data/html\"\n",
        "\n",
        "# define Beautiful Soup key word args\n",
        "bs_kwargs = {\n",
        "    \"features\": \"html.parser\",\n",
        "    \"parse_only\": SoupStrainer(\"p\") # only extract paragraphs\n",
        "}\n",
        "\n",
        "# define Loader key word args\n",
        "loader_kwargs = {\n",
        "    \"open_encoding\": \"utf-8\",\n",
        "    \"bs_kwargs\": bs_kwargs\n",
        "}\n",
        "\n",
        "# define Loader\n",
        "loader = DirectoryLoader(\n",
        "    path=DATA_PATH,\n",
        "    glob=\"*.html\",\n",
        "    loader_cls=BSHTMLLoader,\n",
        "    loader_kwargs=loader_kwargs,\n",
        "    show_progress=True\n",
        "    )\n",
        "\n",
        "# load docs\n",
        "documents = loader.load()\n",
        "\n",
        "# status message\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuEmNwMBVyTe"
      },
      "source": [
        "Next, we perform several simple data cleaning measures by removing line breaks, tabs, and excessive whitespace in order to (1) improve the appearance of the formatted text and (2) prevent excessive whitespace from contributing to the character limit of our query (discussed in more detail below). Lastly, we drop the title label (included by default by LangChain) from our metadata since it is not relevant to our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y09kAWfmVyTe"
      },
      "outputs": [],
      "source": [
        "# clean up document content\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \") # remove line breaks and tabs\n",
        "    doc.page_content = re.sub(\"\\\\s+\", \" \", doc.page_content) # remove excessive whitespace\n",
        "    doc.metadata.pop(\"title\") # drop title labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mHI_vfTVyTe"
      },
      "source": [
        "At this point, we can inspect the first document in our list to better understand the underlying structure of LangChain's `Document` class. Notice that each `Document` consists of two sections: `page_content` and `metadata`. The `page_content` section is quite straightforward since it simply contains the text we just parsed from our HTML files. The `metadata` section contain a single label identifying the 'source' from which the document was retrieved. While the source label conveniently contains the text's FAR reference number, it is formatted as a filepath which may lead to confusion on the part of the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDmIBocqVyTf",
        "outputId": "3905f123-8607-493f-bb1f-60ea216e8536"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Warranties of data shall be developed and used in accordance with agency regulations.', metadata={'source': 'data/html/46.708.html'})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtZNyjZvVyTf"
      },
      "source": [
        "### Modify Source Label\n",
        "\n",
        "A more intuitive way of displaying the source of a given text would be to show the FAR citation as well as the section title. This will tell the user not only where in the FAR to look for further information but also provide a brief description of what the section pertains to so that the user can immediately weigh the relevance of a suggested source. To update our source label, we repeat the process of loading all our documents, however, this time we only extract the titles which contain both the citation and title information. In doing so, we create a list of this information and use it to update the current source labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqyDmvjgVyTf",
        "outputId": "5af58fe4-706b-4d50-a1f6-9de1de01d25c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3487/3487 [00:02<00:00, 1236.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# define Beautiful Soup key word args\n",
        "bs_kwargs = {\n",
        "    \"features\": \"html.parser\",\n",
        "    \"parse_only\": SoupStrainer(\"title\") # only extract titles\n",
        "}\n",
        "\n",
        "# define Loader key word args\n",
        "loader_kwargs = {\n",
        "    \"open_encoding\": \"utf-8\",\n",
        "    \"bs_kwargs\": bs_kwargs\n",
        "}\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    path=DATA_PATH,\n",
        "    glob=\"*.html\",\n",
        "    loader_cls=BSHTMLLoader,\n",
        "    loader_kwargs=loader_kwargs,\n",
        "    show_progress=True\n",
        "    )\n",
        "\n",
        "document_titles = loader.load()\n",
        "\n",
        "# convert source metadata into a list\n",
        "title_list = [doc.metadata[\"title\"] for doc in document_titles]\n",
        "\n",
        "# update source labels\n",
        "i = 0\n",
        "for doc in documents:\n",
        "    doc.metadata[\"source\"] = \" \".join([\"FAR\", title_list[i]])\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LowOI-Q-VyTg"
      },
      "source": [
        "We can inspect our work to evaluate whether these source labels are more intuitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USfkbk5eVyTg",
        "outputId": "44de0c51-d175-4219-a6fd-54485ee1fe72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'source': 'FAR 46.708 Warranties of data.'},\n",
              " {'source': 'FAR 9.405 Effect of listing.'},\n",
              " {'source': 'FAR 11.106 Purchase descriptions for service contracts.'},\n",
              " {'source': 'FAR 16.204 Fixed-price incentive contracts.'},\n",
              " {'source': 'FAR 7.201 [Reserved]'}]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_metadata = [doc.metadata  for doc in documents]\n",
        "doc_metadata[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Df_U0sVyTg"
      },
      "source": [
        "## Create Vector Database\n",
        "\n",
        "Now that we have our documents loaded with metadata labels the way we want them to be, our next step is to store these documents in a vector database. These vector databases work by transforming documents into numerical vector representations using a text embedding model.\n",
        "\n",
        "For this tutorial, we use the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model made available by Hugging Face which maps texts to a 384 dimensional dense vector space. The original document is then stored along with its numerical vector representation and its metadata labels in the vector database. Compared to traditional databases which might return items based on a direct match with key search terms, a query made to a vector database is converted to a numerical vector using the same text embedding model applied to the documents stored within the database. The level of similarity between the numerical vector representation of the query and all the numerical vector representations of the documents is then computed. Typically, [cosine similarity](https://www.geeksforgeeks.org/cosine-similarity/) is the metric used to determine how similar two documents are to one another but [other methods do exist](https://towardsdatascience.com/5-data-similarity-metrics-f358a560855f) and several vector database providers have even developed their own methods for determining similarity.\n",
        "\n",
        "For this tutorial, we use [Chroma](https://www.trychroma.com/), an open-source vector database which [integrates well with LangChain](https://python.langchain.com/docs/integrations/vectorstores/chroma). In just a few lines of code, we specify the embedding model we would like Chroma to use ([all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)), specify the folder we would like our Chroma database files to be stored locally (note: the folder will be created if it does not already exist), and begin uploading our documents to our Chroma database. Please note that prior to creating our Chroma database, we remove any pre-existing Chroma files. By starting fresh each time, we ensure that we do not add duplicate documents to the same database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZj1iEYbVyTh",
        "outputId": "31ec2e63-ac6e-457c-9586-318f4323d8c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 3487 chunks to chroma_db.\n"
          ]
        }
      ],
      "source": [
        "# define embedding model\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "# load embedding function\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
        "\n",
        "# define file path for db\n",
        "CHROMA_PATH = \"chroma_db\"\n",
        "\n",
        "# first, clear out current db\n",
        "if os.path.exists(CHROMA_PATH):\n",
        "    shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "# initialize Chroma db and save locally\n",
        "db = Chroma.from_documents(\n",
        "    documents=documents, embedding=embedding_function, persist_directory=CHROMA_PATH\n",
        "    )\n",
        "db.persist()\n",
        "\n",
        "# status message\n",
        "print(f\"Saved {len(documents)} chunks to {CHROMA_PATH}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B13SmqZ7VyTh"
      },
      "source": [
        "To test whether our vector database is functioning properly, we provide it with a query and see whether Chroma returns the most relevant documents from our database. Here we ask it about the purpose of the FAR and use the `similarity_search_with_relevance_scores()` method with a minimum relevance score threshold of 50% while allowing for up to four documents to be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x352SrlVVyTh",
        "outputId": "d432a82b-f7f7-4994-e5d5-692f7f2f23f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(Document(page_content='The Federal Acquisition Regulations System is established for the codification and publication of uniform policies and procedures for acquisition by all executive agencies. The Federal Acquisition Regulations System consists of the Federal Acquisition Regulation (FAR), which is the primary document, and agency acquisition regulations that implement or supplement the FAR. The FAR System does not include internal agency guidance of the type described in 1.301(a)(2).', metadata={'source': 'FAR 1.101 Purpose.'}),\n",
              "  0.677825443885269),\n",
              " (Document(page_content='This part sets forth basic policies and general information about the Federal Acquisition Regulations System including purpose, authority, applicability, issuance, arrangement, numbering, dissemination, implementation, supplementation, maintenance, administration, and deviation. subparts 1.2,1.3, and 1.4 prescribe administrative procedures for maintaining the FAR System.', metadata={'source': 'FAR 1.000 Scope of part.'}),\n",
              "  0.6351669326334527)]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# query vector db\n",
        "query = \"What is the purpose of the Federal Acquisition Regulations?\"\n",
        "matching_docs = db.similarity_search_with_relevance_scores(\n",
        "    query=query,\n",
        "    k=4, # number of docs to return\n",
        "    score_threshold=.5 # min releavance score required\n",
        "    )\n",
        "matching_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNp-d5DQVyTi"
      },
      "source": [
        "While it may take some time to read through the documents it has returned in order to make assessment, the fact alone that the source label for the document which has been rated most relevant (with a score of 67.8%) contains \"purpose\" in its title is a good indication that our Chroma database is working as intended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5KXhWjEVyTi"
      },
      "source": [
        "<a name=\"rag-pipeline\"></a>\n",
        "# 6. RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpZtOD2dVyTi"
      },
      "source": [
        "Now that we have configured our vector database, we bring together the concepts we have covered so far to build our very own RAG pipeline. The idea is to create a function that will accept a question as input. Based on this question, our vector database will return the five documents calculated to be most relevant to the question at hand. Only documents with a relevance score greater than 50% will be returned. The content from these documents will then be pasted directly into the prompt given to our LLM. We automatically trim content that exceeds this threshold of 1000 words in order to speed up the inference process. Based on the prompt, our LLM should then provide us an informed response. This assumption only holds so long as the question is related to content contained in the vector database. As such, our RAG model will not be able to address questions outside of the scope of the FAR regulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrDJ2CkBVyTi"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n",
        "\n",
        "def RAG(query_text):\n",
        "\n",
        "    # search vector db\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=5, score_threshold=.5)\n",
        "    if len(results) == 0 or results[0][1] < 0.5:\n",
        "        print(f\"Unable to find matching results.\")\n",
        "\n",
        "    # add content from search results to prompt\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "    if len(context_text) > 1000: # handle character limit\n",
        "        context_text = context_text[:1000]\n",
        "        print(\"Warning: Context exceeded 1000 characters, trimming from the end.\")\n",
        "\n",
        "    # define prompt template\n",
        "    prompt_template=PromptTemplate(template=PROMPT_TEMPLATE, input_variables=[\"context\",  \"question\"])\n",
        "\n",
        "    # initialize LLMChain\n",
        "    chain = LLMChain(llm=conv_model, prompt=prompt_template, verbose=True)\n",
        "\n",
        "    # generate response based on context and question\n",
        "    response_text = chain.run({\"context\": context_text, \"question\": query_text})\n",
        "\n",
        "    # extract sources from search results\n",
        "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "\n",
        "    # format and print response with sources\n",
        "    formatted_response = f\"Response: {response_text} \\n Sources: {sources}\"\n",
        "    print(formatted_response)\n",
        "\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEkUIGxEVyTj"
      },
      "source": [
        "We can test our RAG pipeline by passing it a question. Here we ask the model to define the role of the Contracting Officer, a key stakeholder in the public procurement process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFQhUct0VyTj",
        "outputId": "34c8567d-7340-4b71-9ff4-df8831e71b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Context exceeded 1000 characters, trimming from the end.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Answer the question based only on the following context:\n",
            "\n",
            "The contracting officer shall cooperate with Department of Labor representatives in the examination of records, interviews with service employees, and all other aspects of investigations undertaken by the Department. When asked, agencies shall furnish the Wage and Hour Administrator or a designee, any available information on contractors, subcontractors, their contracts, and the nature of the contract services. The contracting officer shall promptly refer, in writing to the appropriate regional office of the Department, apparent violations and complaints received. Employee complaints shall not be disclosed to the employer.\n",
            "\n",
            "---\n",
            "\n",
            "Contracting officers are responsible for ensuring performance of all necessary actions for effective contracting, ensuring compliance with the terms of the contract, and safeguarding the interests of the United States in its contractual relationships. In order to perform these responsibilities, contracting officers should be allowed wide latitude to exercise bu\n",
            "\n",
            "---\n",
            "\n",
            "Answer the question based on the above context: What is a contracting officer?\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: A contracting officer is a government employee who is responsible for administering contracts with contractors or grantees in order to ensure compliance with the contract requirements. They are responsible for negotiating the terms and conditions of the contract, ensuring the timely performance of the contractor, and resolving disputes that may arise during the contract period. \n",
            " Sources: ['FAR 22.1024 Cooperation with the Department of Labor.', 'FAR 1.602-2 Responsibilities.', 'FAR 1.602-1 Authority.', 'FAR 42.601 General.', 'FAR 25.301-3 Weapons.']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'A contracting officer is a government employee who is responsible for administering contracts with contractors or grantees in order to ensure compliance with the contract requirements. They are responsible for negotiating the terms and conditions of the contract, ensuring the timely performance of the contractor, and resolving disputes that may arise during the contract period.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "RAG(\"What is a contracting officer?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX0zpbOZVyTj"
      },
      "source": [
        "Based on the output, the RAG model's output seems informative. The sources it cites appear reasonable, particularly at the start of the list which corresponds with the texts deemed most relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVg6BFRHVyTk"
      },
      "source": [
        "<a name=\"evaluation\"></a>\n",
        "# 7. Model Evaluation\n",
        "\n",
        "Due to the subject matter expertise required to properly evaluate our RAG model's performance, it is difficult to trust our own judgement when it comes to determining how effective the model is. In order to evaluate our model in a more or less objective way, we have sourced fifty questions and answers from training materials that were created with the express purpose of preparing government officials for their procurement licensing examinations. Only questions and answers directly relating to the FAR were included.\n",
        "\n",
        "Some questions and answers have been lightly edited to match the format expected by a chat bot. For example, one question which states \"This type of contract provides for the purchase of supplies or services for more than one, but not more than five program years\" has been re-worded as a question: \"What type of contract provides for the purchase of supplies or services for more than one, but not more than five program years?\" This is not intended to bias our evaluation but rather to make our questions compatible with our open-source LLM.\n",
        "\n",
        "These question and answers have been saved to a spreadsheet which can be found under `data/xlsx/RAG_evaluation_input.xlsx`. Here we show a preview of the types of questions and answers are included in our evaluation. Note that columns for responses and similarity scores are currently empty. These will be populated automatically at a later stage of the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prlgAfM1VyTk",
        "outputId": "21356ea4-7a14-475b-a158-4c8d822328dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50 human-generated questions and answers\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>question_edited</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_edited</th>\n",
              "      <th>response_no_context</th>\n",
              "      <th>response_context</th>\n",
              "      <th>similarity_score_no_context</th>\n",
              "      <th>similarity_score_context</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True or False? Contractors who provide product...</td>\n",
              "      <td>Is it true that contractors who provide produc...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True or False? A contracting officer may unila...</td>\n",
              "      <td>Can a contracting officer unilaterally incorpo...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  True or False? Contractors who provide product...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  True or False? A contracting officer may unila...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                     question_edited  \\\n",
              "0  Is it true that contractors who provide produc...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  Can a contracting officer unilaterally incorpo...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                              answer  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...   \n",
              "1  FAR 1.102-2 provides that the principal custom...   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...   \n",
              "4  FAR 1.701 provides that a “Determination and F...   \n",
              "\n",
              "                                       answer_edited  response_no_context  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...                  NaN   \n",
              "1  FAR 1.102-2 provides that the principal custom...                  NaN   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...                  NaN   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...                  NaN   \n",
              "4  FAR 1.701 provides that a “Determination and F...                  NaN   \n",
              "\n",
              "   response_context  similarity_score_no_context  similarity_score_context  \\\n",
              "0               NaN                          NaN                       NaN   \n",
              "1               NaN                          NaN                       NaN   \n",
              "2               NaN                          NaN                       NaN   \n",
              "3               NaN                          NaN                       NaN   \n",
              "4               NaN                          NaN                       NaN   \n",
              "\n",
              "                                              source  \n",
              "0  https://publiccontractinginstitute.com/far-kno...  \n",
              "1  https://publiccontractinginstitute.com/far-kno...  \n",
              "2  https://publiccontractinginstitute.com/far-kno...  \n",
              "3  https://publiccontractinginstitute.com/far-kno...  \n",
              "4  https://publiccontractinginstitute.com/far-kno...  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load spreadsheet\n",
        "df = pd.read_excel('data/xlsx/RAG_evaluation_input.xlsx')\n",
        "print(f\"{df.shape[0]} human-generated questions and answers\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq9xEqvLVyTk"
      },
      "source": [
        "For each question, we generate both a response from the model *without* context and *with* context. The responses generated without context will serve as our baseline since it represents the response we would expect to have received in the absence of a RAG mechanism. Next, we apply the same text embedding model as we used for our vector database (i.e., [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)) to our human-generated answers to convert them to numerical vectors. We then do the same with the responses generated by the standalone LLM-model (baseline) and the RAG+C enhanced LLM. Using these embeddings, we can then compare these models to the ground truth by computing their respective cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMPGx6AFVyTk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# create list of questions\n",
        "question_list = df[\"question_edited\"]\n",
        "\n",
        "# generate baseline responses (not based on context)\n",
        "baseline_response_list = [conv_chain.run(question) for question in question_list]\n",
        "\n",
        "# generate responses (based on context)\n",
        "response_list = [RAG(question) for question in question_list]\n",
        "\n",
        "# create list of answers\n",
        "answer_list = df[\"answer_edited\"]\n",
        "\n",
        "# load embedding model\n",
        "embedding_model = SentenceTransformer(model_name_or_path=EMBED_MODEL)\n",
        "\n",
        "# convert answers to embeddings\n",
        "answer_embeddings = [embedding_model.encode(answer) for answer in answer_list]\n",
        "\n",
        "# compute embeddings for responses without context\n",
        "baseline_response_embeddings = []\n",
        "for response in baseline_response_list:\n",
        "    if response is None:\n",
        "        embedding = pd.NA\n",
        "    else:\n",
        "        embedding = embedding_model.encode(response)\n",
        "    baseline_response_embeddings.append(embedding)\n",
        "\n",
        "# compute embeddings for responses with context\n",
        "response_embeddings = []\n",
        "for response in response_list:\n",
        "    if response is None:\n",
        "        embedding = pd.NA\n",
        "    else:\n",
        "        embedding = embedding_model.encode(response)\n",
        "    response_embeddings.append(embedding)\n",
        "\n",
        "# reshape embeddings to 2D arrays (required by cosine_similarity)\n",
        "answer_embeddings = [np.reshape(embedding, (1, -1)) for embedding in answer_embeddings]\n",
        "response_embeddings = [np.reshape(embedding, (1, -1)) for embedding in response_embeddings]\n",
        "baseline_response_embeddings = [np.reshape(embedding, (1, -1)) for embedding in baseline_response_embeddings]\n",
        "\n",
        "# calculate similarity scores\n",
        "# questions vs responses with context\n",
        "similarity_scores = []\n",
        "for a_embedding, r_embedding in zip(answer_embeddings, response_embeddings):\n",
        "    if pd.isna(r_embedding).any():\n",
        "        score = pd.NA\n",
        "    else:\n",
        "        score = cosine_similarity(a_embedding, r_embedding).item()\n",
        "    similarity_scores.append(score)\n",
        "\n",
        "# questions vs responses without context\n",
        "baseline_similarity_scores = []\n",
        "for a_embedding, br_embedding in zip(answer_embeddings, baseline_response_embeddings):\n",
        "    if pd.isna(br_embedding).any():\n",
        "        score = pd.NA\n",
        "    else:\n",
        "        score = cosine_similarity(a_embedding, br_embedding).item()\n",
        "    baseline_similarity_scores.append(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2jCbQelVyTl"
      },
      "source": [
        "Lastly, we add the generated responses and corresponding similarity scores to our dataframe. Below you will find a glimpse of the populated dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5JZicWoVyTl",
        "outputId": "400e628b-cee6-4e64-e4f7-adb34c7abc47"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>question_edited</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_edited</th>\n",
              "      <th>response_no_context</th>\n",
              "      <th>response_context</th>\n",
              "      <th>similarity_score_no_context</th>\n",
              "      <th>similarity_score_context</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True or False? Contractors who provide product...</td>\n",
              "      <td>Is it true that contractors who provide produc...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...</td>\n",
              "      <td>According to Federal Acquisition Regulation (F...</td>\n",
              "      <td>No</td>\n",
              "      <td>0.719175</td>\n",
              "      <td>0.084247</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>Who are the principal customers for the produc...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>FAR 1.102-2 provides that the principal custom...</td>\n",
              "      <td>The principal customers for the products and s...</td>\n",
              "      <td>The Federal Acquisition System (FAS) serves as...</td>\n",
              "      <td>0.455638</td>\n",
              "      <td>0.244849</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True or False? A contracting officer may unila...</td>\n",
              "      <td>Can a contracting officer unilaterally incorpo...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>False. FAR 1.108(d)(3) provides that “Contract...</td>\n",
              "      <td>No, it is not common for a contracting officer...</td>\n",
              "      <td>As an AI language model, I cannot provide lega...</td>\n",
              "      <td>0.659838</td>\n",
              "      <td>0.645821</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>What three government officials are personally...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>FAR 1.202 provides that “Agency compliance wit...</td>\n",
              "      <td>1. Secretary of Defense\\n2. Chief of Staff\\n3....</td>\n",
              "      <td>\\nThe three government officials responsible f...</td>\n",
              "      <td>0.509940</td>\n",
              "      <td>0.533886</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>What is a D&amp;F?</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>FAR 1.701 provides that a “Determination and F...</td>\n",
              "      <td>I'm sorry, but I am not familiar with the term...</td>\n",
              "      <td>\\nThe question is ambiguous as it does not spe...</td>\n",
              "      <td>0.107604</td>\n",
              "      <td>0.183559</td>\n",
              "      <td>https://publiccontractinginstitute.com/far-kno...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  True or False? Contractors who provide product...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  True or False? A contracting officer may unila...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                     question_edited  \\\n",
              "0  Is it true that contractors who provide produc...   \n",
              "1  Who are the principal customers for the produc...   \n",
              "2  Can a contracting officer unilaterally incorpo...   \n",
              "3  What three government officials are personally...   \n",
              "4                                     What is a D&F?   \n",
              "\n",
              "                                              answer  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...   \n",
              "1  FAR 1.102-2 provides that the principal custom...   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...   \n",
              "4  FAR 1.701 provides that a “Determination and F...   \n",
              "\n",
              "                                       answer_edited  \\\n",
              "0  True. Both FAR 1.102(b)(c) and FAR 1.102-3 not...   \n",
              "1  FAR 1.102-2 provides that the principal custom...   \n",
              "2  False. FAR 1.108(d)(3) provides that “Contract...   \n",
              "3  FAR 1.202 provides that “Agency compliance wit...   \n",
              "4  FAR 1.701 provides that a “Determination and F...   \n",
              "\n",
              "                                 response_no_context  \\\n",
              "0  According to Federal Acquisition Regulation (F...   \n",
              "1  The principal customers for the products and s...   \n",
              "2  No, it is not common for a contracting officer...   \n",
              "3  1. Secretary of Defense\\n2. Chief of Staff\\n3....   \n",
              "4  I'm sorry, but I am not familiar with the term...   \n",
              "\n",
              "                                    response_context  \\\n",
              "0                                                 No   \n",
              "1  The Federal Acquisition System (FAS) serves as...   \n",
              "2  As an AI language model, I cannot provide lega...   \n",
              "3  \\nThe three government officials responsible f...   \n",
              "4  \\nThe question is ambiguous as it does not spe...   \n",
              "\n",
              "   similarity_score_no_context  similarity_score_context  \\\n",
              "0                     0.719175                  0.084247   \n",
              "1                     0.455638                  0.244849   \n",
              "2                     0.659838                  0.645821   \n",
              "3                     0.509940                  0.533886   \n",
              "4                     0.107604                  0.183559   \n",
              "\n",
              "                                              source  \n",
              "0  https://publiccontractinginstitute.com/far-kno...  \n",
              "1  https://publiccontractinginstitute.com/far-kno...  \n",
              "2  https://publiccontractinginstitute.com/far-kno...  \n",
              "3  https://publiccontractinginstitute.com/far-kno...  \n",
              "4  https://publiccontractinginstitute.com/far-kno...  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add responses and similarity scores to df\n",
        "df[\"response_context\"] = response_list\n",
        "df[\"response_no_context\"] = baseline_response_list\n",
        "df[\"similarity_score_no_context\"] = baseline_similarity_scores\n",
        "df[\"similarity_score_context\"] = similarity_scores\n",
        "\n",
        "# save xlslx\n",
        "df.to_excel(\"data/xlsx/RAG_evaluation_output.xlsx\")\n",
        "\n",
        "# inspect df\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxw9ZqDGVyTm"
      },
      "source": [
        "## Results & Discussion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSsZs0ICVyTm",
        "outputId": "42c685c3-4621-4d7e-b539-4c22b364dbb8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>avg_similarity_score</th>\n",
              "      <th>response_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline</td>\n",
              "      <td>0.318718</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RAG</td>\n",
              "      <td>0.363935</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      model  avg_similarity_score  response_rate\n",
              "0  Baseline              0.318718          100.0\n",
              "1       RAG              0.363935          100.0"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create summary table\n",
        "summary_df = pd.DataFrame({\n",
        "    \"model\": [\"Baseline\", \"RAG\"],\n",
        "    \"avg_similarity_score\": [pd.Series(baseline_similarity_scores).mean(skipna=True),\n",
        "                             pd.Series(similarity_scores).mean(skipna=True)],\n",
        "    \"response_rate\": [(pd.Series(baseline_similarity_scores).count() / len(baseline_similarity_scores) * 100),\n",
        "                    (pd.Series(similarity_scores).count() / len(similarity_scores) * 100)]\n",
        "                    })\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZtGaJqGVyTn"
      },
      "source": [
        "By comparing the average similarity scores between the human-generated answer and our respective responses, we can see that the RAG model produces responses with a higher level of semantic similarity than the baseline model, supporting our hypothesis that additional external context improves the relevance of responses. While we cannot draw any definitive conclusions about the precision and accuracy of our RAG model using semantic similarity as an evaluation method, we can at least consider the model to be better-performing relative to the baseline.\n",
        "\n",
        "These results, and particularly the absolute similarity score should be treated with care. One concern relates to extent to which the similarity score also picks up differences in style, additional to information truth. Put differently, a model might get the information perfectly right in its answer, however, if it frames it vastly differently to the ground truth, the associated similarity score might still be low. However, since this dynamic should affect both the baseline and RAG model similarly, a relative comparison of the two still seems valid. Nevertheless, an extension of this evaluation would not only develop a broader set of validation questions, but should ideally also rely on human evaluators with domain knowledge to assess the accuracy of the models' answers.\n",
        "\n",
        "Independent of any concerns regarding accurate measurement, there exist obvious avenues to boost model performance. For instance, one might treat the separation of the FAR handbook into text chunks as a hyperparamter, whereby greater granularity could improve the relevance of content returned by the vector database, although this may come at the expense of losing additional context. More testing would be required to determine what level of granularity optimizes the quality of the model response. Furthermore, our pipeline is limited to using free and open-source models even while better LLMs do exist. A straightforward way of improving pipeline performance could be to use a paid option such as ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhkmytKNU_Z2"
      },
      "source": [
        "<a name=\"cui-deployment\"></a>\n",
        "# 8. Deployment as Conversation User Interface (CUI)\n",
        "\n",
        "Deploying the model as a Conversation User Interface (CUI) necessitates embedding the model in a deployment framework and running it on a web server. Given these technical intricacies, we cannot run the code as part of this notebook. Instead, we'll provide selected bits of the code and provide and overview of key considerations, as well as sharing the access to the repository from which we host the application for you to explore on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Fm2YhrVyTn"
      },
      "source": [
        "## Key Steps and Considerations\n",
        "\n",
        "- **Utilizing Chainlit Framework:** We employ Chainlit to convert the model into a web application. Chainlit, an asynchronous Python framework, supports numerous concurrent connections and operates on an event-driven basis. This setup allows for efficient processing and handling of user interactions. Using Chainlit's event-driven syntax, specific code segments are linked to particular user actions. For instance, when a user initiates a chat, the system loads and processes relevant documents, establishes the vector database, and loads the model. Subsequently, when a user poses a question, it feeds into the model as a query, computes a response, and presents associated sources.\n",
        "\n",
        "- **Adaptating RAG Pipeline for Compatibility with Chainlit:** To align with Chainlit's requirements, the model undergoes slight modifications, employing the `ConversationRetrievalChain`, which is highly tailored to the task. While this simplifies deployment, it sacrifices some flexibility and transparency compared to the earlier RAG versions you encountered. We made this choice consciously, as we wanted you to grasp what is happening under the hood first, before employing the more effective, but also opaque ready-made function.\n",
        "\n",
        "- **Chainlit Design Features:** Chainlit comes with many useful and intuitive design features. For instance, you may easily design and adapt your very own landing page accompanying your application. This can be done by manipulating the `chainlit.md` file which is created when running your application for the first time. Check out the landing page for our FAR-Chat in our repository!\n",
        "\n",
        "- **Deployment via Hugging Face Spaces:** Hosting the application on a personal web server restricts access. Therefore, we utilize Hugging Face Spaces, a free, open-source service from Hugging Face, for public deployment. The platform's interface resembles GitHub and simplifies the upload of data and applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDqFXgF8VyTn"
      },
      "source": [
        "## Chainlit Python Script\n",
        "The script below gives you a glimpse into the changes the pipeline has undergone when being transformed into a deployable python application. Please note, that the code chunk cannot be run as part of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LWo6UQzwj37",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "# import all necessary packages\n",
        "import os\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import BSHTMLLoader\n",
        "from bs4 import SoupStrainer\n",
        "import re\n",
        "\n",
        "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
        "\n",
        "import chainlit as cl\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# define prompt template\n",
        "system_template = \"\"\"Use the following pieces of context to answer the users question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "ALWAYS return a \"SOURCES\" part in your answer.\n",
        "The \"SOURCES\" part should be a reference to the source of the document from which you got your answer.\n",
        "And if the user greets with greetings like Hi, hello, How are you, etc reply accordingly as well.\n",
        "Example of your response should be:\n",
        "The answer is foo\n",
        "SOURCES: xyz\n",
        "Begin!\n",
        "----------------\n",
        "{summaries}\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "]\n",
        "prompt = ChatPromptTemplate.from_messages(messages)\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "\n",
        "# define the llm\n",
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "conv_model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=os.environ['HF_API_TOKEN'],\n",
        "    repo_id=model_id,\n",
        "    model_kwargs={\"temperature\":0.8,\"max_length\": 1000}\n",
        "    )\n",
        "\n",
        "# set up vector db with chroma\n",
        "data_path = \"data/html\"\n",
        "embed_model = \"all-MiniLM-L6-v2\" # Chroma defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# load documents\n",
        "def load_documents(directory):\n",
        "\n",
        "    # define Beautiful Soup key word args\n",
        "    bs_kwargs = {\n",
        "        \"features\": \"html.parser\",\n",
        "        \"parse_only\": SoupStrainer(\"p\") # only include relevant text\n",
        "        }\n",
        "\n",
        "    # define Loader key word args\n",
        "    loader_kwargs = {\n",
        "        \"open_encoding\": \"utf-8\",\n",
        "        \"bs_kwargs\": bs_kwargs\n",
        "        }\n",
        "\n",
        "    # define Loader\n",
        "    loader = DirectoryLoader(\n",
        "        path=directory,\n",
        "        glob=\"*.html\",\n",
        "        loader_cls=BSHTMLLoader,\n",
        "        loader_kwargs=loader_kwargs\n",
        "        )\n",
        "\n",
        "    documents = loader.load()\n",
        "    return documents\n",
        "\n",
        "\n",
        "# prepare documents\n",
        "def prepare_documents(documents):\n",
        "    for doc in documents:\n",
        "        doc.page_content = doc.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        doc.page_content = re.sub(\"\\\\s+\", \" \", doc.page_content)\n",
        "\n",
        "    # define Beautiful Soup key word args\n",
        "    bs_kwargs = {\n",
        "        \"features\": \"html.parser\",\n",
        "        \"parse_only\": SoupStrainer(\"title\") # only include relevant text\n",
        "        }\n",
        "\n",
        "    # define Loader key word args\n",
        "    loader_kwargs = {\n",
        "        \"open_encoding\": \"utf-8\",\n",
        "        \"bs_kwargs\": bs_kwargs\n",
        "        }\n",
        "\n",
        "    loader = DirectoryLoader(\n",
        "        path=data_path,\n",
        "        glob=\"*.html\",\n",
        "        loader_cls=BSHTMLLoader,\n",
        "        loader_kwargs=loader_kwargs\n",
        "        )\n",
        "\n",
        "    document_sources = loader.load()\n",
        "\n",
        "    # convert source metadata into a list\n",
        "    source_list = [doc.metadata[\"title\"] for doc in document_sources]\n",
        "\n",
        "    # update source metadata\n",
        "    i = 0\n",
        "    for doc in documents:\n",
        "        doc.metadata[\"source\"] = \" \".join([\"FAR\", source_list[i]])\n",
        "        i += 1\n",
        "    return documents\n",
        "\n",
        "# define a function to execute when a chat starts\n",
        "@cl.on_chat_start\n",
        "async def on_chat_start():\n",
        "    # instantiate the chain for that user session\n",
        "    embedding_func = SentenceTransformerEmbeddings(model_name=embed_model)\n",
        "\n",
        "    # display a message indicating document loading\n",
        "    msg = cl.Message(\n",
        "        content=\"Loading and processing documents. This may take a while...\",\n",
        "        disable_human_feedback=True)\n",
        "    await msg.send()\n",
        "\n",
        "    # load and prepare documents for processing\n",
        "    documents = load_documents(data_path)\n",
        "    documents = prepare_documents(documents)\n",
        "\n",
        "    # create a document search object asynchronously\n",
        "    docsearch = await cl.make_async(Chroma.from_documents)(\n",
        "        documents,\n",
        "        embedding_func\n",
        "    )\n",
        "\n",
        "    # initialize ChatMessageHistory object to store message history\n",
        "    message_history = ChatMessageHistory()\n",
        "\n",
        "    # initialize ConversationBufferMemory object to store conversation history\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        output_key=\"answer\",\n",
        "        chat_memory=message_history,\n",
        "        return_messages=True,\n",
        "    )\n",
        "\n",
        "    # create a ConversationalRetrievalChain object\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        conv_model,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=docsearch.as_retriever(),\n",
        "        memory=memory,\n",
        "        return_source_documents=True,\n",
        "    )\n",
        "\n",
        "    # indicate readiness for questions\n",
        "    msg.content = \"Ready. You can now ask questions!\"\n",
        "    await msg.update()\n",
        "\n",
        "    # store the chain in the user's session\n",
        "    cl.user_session.set(\"chain\", chain)\n",
        "\n",
        "# define a function to handle messages\n",
        "@cl.on_message\n",
        "async def main(message):\n",
        "    # retrieve the chain object from the user's session\n",
        "    chain = cl.user_session.get(\"chain\")  # type: ConversationalRetrievalChain\n",
        "    cb = cl.AsyncLangchainCallbackHandler()\n",
        "\n",
        "    # call the chain to process the incoming message\n",
        "    res = await chain.acall(message.content, callbacks=[cb])\n",
        "\n",
        "    # retrieve the answer and source documents from the chain's response\n",
        "    answer = res[\"answer\"]\n",
        "    source_documents = res[\"source_documents\"]\n",
        "\n",
        "    text_elements = []  # list to store text elements\n",
        "    source_names = set()  # set to store unique source names\n",
        "\n",
        "    # iterate through source documents and extract relevant information\n",
        "    for idx, source_doc in enumerate(source_documents):\n",
        "        source_name = source_doc.metadata[\"source\"]\n",
        "        text_elements.append(\n",
        "                cl.Text(content=source_doc.page_content,\n",
        "                        name=source_name))\n",
        "        source_names.add(source_name)  # add the source name to the set\n",
        "\n",
        "    # append sources information to the answer if available\n",
        "    if source_names:\n",
        "            answer += f\"\\nSources: {', '.join(source_names)}\"\n",
        "    else:\n",
        "            answer += \"\\nNo sources found\"\n",
        "\n",
        "    # send the answer along with any extracted text elements\n",
        "    await cl.Message(content=answer, elements=text_elements).send()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYDkQ79iVyTp"
      },
      "source": [
        "## Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsLy-IY2VyTq"
      },
      "source": [
        "#### Local Hosting:\n",
        "You can effortlessly host the application locally through the terminal. Just execute `chainlit run [filename.py] -w` in a terminal conencted to local repository. This command opens a locally hosted server which is displayed in your default internet browser. By including `-w`, any modifications made to the script will automatically update the associated image.\n",
        "\n",
        "#### Sharing via Web Server:\n",
        "For broader accessibility to the application, utilizing a web server becomes pivotal. Here's where Hugging Face Spaces comes in. The Hugging Face Spaces repository mirrors your local repository, containg the python application and any data. Additionally, you have to create a Docker file, which contains the instructions to build a Docker image, which in turn allows you to deploy your python application using containers. Importantly, the Docker file for chainlit deployment in Hugging Face Spaces is largely standardized, and you will only have to change the name of the Python file associated with your application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ZwlBNZVyTq"
      },
      "source": [
        "### Hugging Face Spaces Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2Pfxfh2KVyTq",
        "outputId": "6c043e6b-1e66-4af6-903a-2eddcaf05551"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/hugging-face-repo.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/hugging-face-repo.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWQsJUG9VyTr"
      },
      "source": [
        "### Docker File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A5XHyexbVyTr",
        "outputId": "5dbc450a-b1ed-4166-a01f-20fea58f62c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/docker-file.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/docker-file.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJLZKhQHVyTr"
      },
      "source": [
        "## Access our Federal Acquisition Regulation (FAR) Chat\n",
        "Access our \"FAR-Chat\" on Hugging Face Spaces [here](https://huggingface.co/spaces/smkerr/rag-chat). Explore the repository and its files, especially the adapted Python application [here](https://huggingface.co/spaces/smkerr/rag-chat/tree/main)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/far-chat-preview.png\" style=\"width: 55%\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/smkerr/tutorial-RAG-C/main/img/far-chat-preview.png'\n",
        "HTML(f'<img src=\"{image_path}\" style=\"width: {image_width_percentage}%\">')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGydMxgcVyTs"
      },
      "source": [
        "## Caveats on Usage and Performance\n",
        "\n",
        "- **Limitations of Hugging Face Spaces:** Hugging Face Spaces' free access comes with constrained CPU support. Consequently, working with the entire dataset of nearly 3,500 documents led to significant waiting times upon launching the chat. Thus, a random subset of 500 files was chosen to showcase functionality.\n",
        "\n",
        "- **Model Performance:** As a result of the smaller dataset and  performance limitations of open-source models like Falcon-7b-instruct compared to paid models like GPT-4, occasional erroneous or irrelevant answers might occur. Restarting the chat or rephrasing the question often rectifies such issues.\n",
        "\n",
        "- **Enhancing Performance:** A small investment in GPU-supported web hosting and premium model access could substantially improve performance, mitigating these limitations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKqewK_-ZLD"
      },
      "source": [
        "<a name=\"references\"></a>\n",
        "# References & Further Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in-ejZs9VyTs"
      },
      "source": [
        "## References\n",
        "\n",
        "The following resources were consulted along with ChatGPT and Stack Overflow to explain, troubleshoot, and comment on the code:\n",
        "\n",
        "* 9/11 Comission. 2004. Final Report. Link: https://www.9-11commission.gov/report/911Report.pdf\n",
        "* Best Open Source LLM — Falcon 40B Chatbot in LangChain: https://youtu.be/ukj_ITJKBwE?si=t-srzn5YgcCiLjiC\n",
        "* Better Llama 2 with Retrieval Augmented Generation (RAG): https://youtu.be/ypzmPwLH_Q4?si=qVRzr4b95sMk_XBc\n",
        "* Build a Private Chatbot with Local LLM (Falcon 7B) and LangChain: https://youtu.be/N7dGOUwufBM?si=c2htR20tc-1eC33V\n",
        "* Building RAG Chatbots with LangChain: https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb\n",
        "* Chatbots with RAG: LangChain Full Walkthrough: https://youtu.be/LhnCsygAvzY?si=bXJvLknL1Z5rIxR_\n",
        "* Chroma Documentation: Hugging Face: https://docs.trychroma.com/embeddings/hugging-face\n",
        "* Chroma Documentation: https://docs.trychroma.com/embeddings\n",
        "* Complete LangChain Tutorial: https://github.com/krishnaik06/Complete-Langchain-Tutorials\n",
        "* Embeddings and Vector Databases With ChromaDB: https://realpython.com/chromadb-vector-database/\n",
        "* Get Things Done with Prompt Engineering and LangChain: https://github.com/curiousily/Get-Things-Done-with-Prompt-Engineering-and-LangChain\n",
        "* Hugging Face: setence-transformers: https://huggingface.co/sentence-transformers\n",
        "* Hugging Face: sentence-transformers/all-MiniLM-L6-v2: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "* LangChain Documentation: Chat Models: https://python.langchain.com/docs/integrations/chat/\n",
        "* LangChain Documentation: Chroma: https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
        "* LangChain Documentation: File Directory: https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory\n",
        "* LangChain Documentation: HTML: https://python.langchain.com/docs/modules/data_connection/document_loaders/html#loading-html-with-beautifulsoup4\n",
        "* Langchain RAG Tutorial: https://github.com/pixegami/langchain-rag-tutorial\n",
        "* Learn LangChain In 1 Hour With End To End LLM Project With Deployment In Hugging Face Spaces: https://youtu.be/qMIM7dECAkc?si=JcHClnCbKjjCwFKf\n",
        "* Partnership for Public Service. 2019. Federal Workforce. Link: https://ourpublicservice.org/wp-content/uploads/2022/03/FedFigures_FY18-Workforce-1.pdf\n",
        "* Private Chatbot with Local LLM (Falcon 7B) and LangChain: https://www.mlexpert.io/prompt-engineering/chatbot-with-local-llm-using-langchain\n",
        "* RAG + Langchain Python Project: https://youtu.be/tcqEUSNCn8I?si=xT0EdUocHfiiSO9z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKHPEGh9VyTs"
      },
      "source": [
        "## Further Resources\n",
        "\n",
        "This tutorial is designed to give you a thorough introduction to RAG+Cs. While the tutorial focuses more on their hands-on implementation, these resources extend and deepen the content covered in the tutorial through conceptual underpinnings from academia and technical experts.\n",
        "\n",
        "* [Seminal paper on RAG introduced at the 34th NeurIPS in 2020](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf): Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuettler, H., Lewis, M., Yih, W., Rocktaeschel, T., Riedel, S and Kiela, D. (2020)\n",
        "* [Here is a presentation of the above paper by the author](https://www.youtube.com/watch?v=JGpmQvlYRdU): Lewis, P. (2020)\n",
        "* [More on obstacles for RAG and potential remedies](https://aclanthology.org/2022.naacl-srw.7/): Yu, W. (2022)\n",
        "* [Blog post on evaluating RAG models](https://towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de): Besbes, A. (2023, Nov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHWzscJVyTt"
      },
      "source": [
        "For a straightforward, hands-on introduction to RAG, check out this video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icll8yL3VyTt",
        "outputId": "e28ad894-59e6-440d-a413-1976931d9d5b"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/LhnCsygAvzY\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x107b733d0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YouTubeVideo('LhnCsygAvzY')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
